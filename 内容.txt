在环境感知领域，多篇论文聚焦 LiDAR 相关攻击与防御。如《Random Spoofing Attack against LiDAR-Based Scan Matching SLAM》首次系统评估 LiDAR Scan Matching SLAM 在物理世界下对异步激光欺骗攻击的脆弱性，利用异步随机激光注入技术绕过检测机制，干扰地图构建 ；《SLAMSpoof: Practical LiDAR Spoofing Attacks on Localization Systems Guided by Scan Matching Vulnerability Analysis》提出首个可实用的 LiDAR 定位欺骗攻击系统 SLAMSpoof，并探讨多传感器融合等潜在防御方法。此外，《BadDepth: Backdoor Attacks Against Monocular Depth Estimation in the Physical World》提出 BadDepth 攻击方法，专门针对单目深度估计任务实施后门攻击；《A Survey on Adversarial Robustness of LiDAR-based Machine Learning Perception in Autonomous Vehicles》则对基于 LiDAR 的感知系统威胁进行综述，梳理传感器网络攻击与对抗扰动等威胁及防御策略；《Adversarial Attacks and Detection in Visual Place Recognition for Safer Robot Navigation》研究四种通用及四种针对视觉定位系统（VPR）的新攻击方式，并引入主动防御。​

决策规划方向的研究集中于路径规划与任务规划的攻击。《Characterizing Physical Adversarial Attacks on Robot Motion Planners》首次提出对运动规划器的 “planner failure” 和 “blindspot” 两类物理环境攻击；《Manipulating Neural Path Planners via Slight Perturbations》在神经路径规划器中植入可被微小扰动触发的后门；《Exploring Adversarial Obstacle Attacks in Search-based Path Planning for Autonomous Mobile Robots》针对搜索式路径规划算法，提出 “Obstacle Attacks” 威胁模型，通过篡改地图影响规划效率 。针对任务规划，《Robo-Troj: Attacking LLM-based Task Planners》提出首个针对 LLM 任务规划的后门攻击 Robo-Troj；《BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization》揭示 VLA 模型后门漏洞并提出 BadVLA 攻击方法；《Adversarial Attacks on Robotic Vision-Language-Action Models》将大语言模型 “越狱攻击” 技术迁移到 VLA 模型，实现完全控制权。​

物理交互方面，《Optimal Actuator Attacks on Autonomous Vehicles Using Reinforcement Learning》提出面向非线性系统的最优执行器 FDI 攻击策略，并设计泛化能力强的安全控制器；《Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and Attacks》对执行器安全威胁进行系统总结。此外，《Strategic Planning of Stealthy Backdoor Attacks in Markov Decision Processes》在随机控制系统中设计可通过测试的隐蔽后门攻击策略。​

当前机器人安全研究已在环境感知、决策规划和物理交互等多方面展开，攻击方法涵盖后门植入、对抗扰动、物理欺骗等，部分研究也提出了相应防御策略。然而，研究仍存在不足，如防御技术成熟度普遍低于攻击技术，部分领域缺乏系统性的防御方案，未来需要更多研究聚焦于提升机器人系统的整体安全性与鲁棒性。​

英文版：
In the field of environmental perception, numerous studies have focused on LiDAR-related attacks and defenses. For example, "Random Spoofing Attack against LiDAR-Based Scan Matching SLAM" is the first to systematically evaluate the vulnerability of LiDAR scan matching SLAM to asynchronous laser spoofing attacks in the physical world. It uses asynchronous random laser injection techniques to bypass detection mechanisms and disrupt map construction. "SLAMSpoof: Practical LiDAR Spoofing Attacks on Localization Systems Guided by Scan Matching Vulnerability Analysis" proposes SLAMSpoof, the first practical LiDAR localization spoofing attack system, and explores potential defense strategies such as multi-sensor fusion. In addition, "BadDepth: Backdoor Attacks Against Monocular Depth Estimation in the Physical World" introduces the BadDepth attack method, which targets monocular depth estimation tasks through backdoor attacks. "A Survey on Adversarial Robustness of LiDAR-based Machine Learning Perception in Autonomous Vehicles" provides a comprehensive review of threats to LiDAR-based perception systems, covering sensor network attacks, adversarial perturbations, and corresponding defense strategies. "Adversarial Attacks and Detection in Visual Place Recognition for Safer Robot Navigation" studies four generic and four novel attack methods targeting visual place recognition (VPR) systems and proposes proactive defense techniques.

Research on decision-making and planning mainly focuses on attacks against path planning and task planning. "Characterizing Physical Adversarial Attacks on Robot Motion Planners" is the first to propose two types of physical environment attacks—“planner failure” and “blindspot”—on motion planners. "Manipulating Neural Path Planners via Slight Perturbations" implants backdoors in neural path planners that can be triggered by slight perturbations. "Exploring Adversarial Obstacle Attacks in Search-based Path Planning for Autonomous Mobile Robots" proposes an "Obstacle Attacks" threat model against search-based path planning algorithms by tampering with maps to impact planning efficiency. For task planning, "Robo-Troj: Attacking LLM-based Task Planners" introduces Robo-Troj, the first backdoor attack targeting LLM-based task planners. "BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization" exposes backdoor vulnerabilities in VLA models and proposes the BadVLA attack. "Adversarial Attacks on Robotic Vision-Language-Action Models" transfers jailbreak attack techniques from large language models (LLMs) to VLA models, achieving full control over them.

In terms of physical interaction, "Optimal Actuator Attacks on Autonomous Vehicles Using Reinforcement Learning" proposes an optimal actuator FDI (false data injection) attack strategy for nonlinear systems and designs a robust safety controller with strong generalization. "Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and Attacks" systematically summarizes actuator security threats. Additionally, "Strategic Planning of Stealthy Backdoor Attacks in Markov Decision Processes" designs stealthy backdoor attack strategies that can pass testing in stochastic control systems.

In conclusion, attack methods include backdoor implantation, adversarial perturbations, and physical spoofing, with some studies also proposing corresponding defense strategies. However, limitations remain: for example, defense techniques are generally less mature than attack techniques, and some domains still lack systematic defense solutions. Future research should focus on enhancing the overall security and robustness of robotic systems.