(function(){var n={3562:function(n){n.exports=[{"标题":"A Model-Agnostic Approach for Semantically Driven Disambiguation in Human-Robot Interaction","文章简称":"Fethiye Irmak et al.","\\cite{}":"dogan2025modelagnosticapproachsemanticallydriven","发表年月":25.04,"10大原则":"可信-准确","4个阶段":"指令理解","一作":"Fethiye Irmak Dogan","一作单位":"KTH Royal Institute of Technology","通讯":null,"通讯单位":null,"具身任务":"Object Search","方法论":"LLM, 迭代式交互, 不确定度","发表情况":"IEEE International Conference on Robot & Human Interactive Communication (RO-MAN)","链接":"https://arxiv.org/abs/2409.17004","备注/讨论点":"关键词：指令歧义；任务：自然语言描述的定位，不是3D Grounding","父记录":null},{"标题":"AGENTSAFE: Benchmarking the Safety of Embodied Agents on Hazardous Instructions","文章简称":"AGENTSAFE","\\cite{}":"liu2025agentsafebenchmarkingsafetyembodied","发表年月":25.06,"10大原则":"安全-抗攻击","4个阶段":"指令理解","一作":"Aishan Liu","一作单位":"BUAA","通讯":null,"通讯单位":null,"具身任务":"EQA(2D), VLA, VLN, Planning","方法论":"Benchmark, MLLM, Attack Paradigm, AI2-THOR","发表情况":null,"链接":"https://arxiv.org/abs/2506.14697","备注/讨论点":null,"父记录":null},{"标题":"Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation","文章简称":"EAsafetyBench","\\cite{}":"wang2025advancingembodiedagentsecurity","发表年月":25.04,"10大原则":"安全-抗攻击","4个阶段":"指令理解","一作":"Ning Wang","一作单位":"CQU","通讯":"Chuan Ma","通讯单位":"CQU","具身任务":"EQA(2D)","方法论":"Benchmark","发表情况":null,"链接":"https://arxiv.org/abs/2504.15699","备注/讨论点":null,"父记录":null},{"标题":"Adversarial Attacks on Robotic Vision Language Action Models","文章简称":"Eliot Krzysztof et al.","\\cite{}":"jones2025adversarialattacksroboticvision","发表年月":25.06,"10大原则":"安全-抗攻击","4个阶段":"指令理解","一作":"Eliot Krzysztof Jones","一作单位":"Gray Swan AI","通讯":"Eliot Krzysztof Jones1","通讯单位":"Gray Swan AI","具身任务":"VLA","方法论":"adversarial attack","发表情况":null,"链接":"https://arxiv.org/abs/2506.03350","备注/讨论点":null,"父记录":null},{"标题":"Adversarial Training for Multimodal Large Language Models against Jailbreak Attacks","文章简称":"ProEAT","\\cite{}":"lu2025adversarialtrainingmultimodallarge","发表年月":25.03,"10大原则":"安全-抗攻击","4个阶段":"指令理解","一作":"Liming Lu","一作单位":"NJUST","通讯":null,"通讯单位":null,"具身任务":"EQA(2D)","方法论":"MLLM, adversarial attack, 指令微调","发表情况":null,"链接":"https://arxiv.org/abs/2503.04833","备注/讨论点":null,"父记录":null},{"标题":"BadNAVer: Exploring Jailbreak Attacks On Vision-and-Language Navigation","文章简称":"BadNAVer","\\cite{}":"lyu2025badnaverexploringjailbreakattacks","发表年月":25.05,"10大原则":"安全-抗攻击","4个阶段":"指令理解","一作":"Wenqi Lyu","一作单位":"AIML","通讯":"Qi Wu","通讯单位":"AIML","具身任务":"VLN","方法论":"MLLM","发表情况":null,"链接":"https://arxiv.org/abs/2505.12443","备注/讨论点":null,"父记录":null},{"标题":"Concept Enhancement Engineering: A Lightweight and Efficient Robust Defense Against Jailbreak Attacks in Embodied AI","文章简称":"CEE","\\cite{}":"yang2025conceptenhancementengineeringlightweight","发表年月":25.04,"10大原则":"安全-抗攻击","4个阶段":"指令理解","一作":"Jirui Yang; Zheyu Lin","一作单位":"FDU","通讯":null,"通讯单位":null,"具身任务":"Planning, EQA(2D)","方法论":"MLLM","发表情况":null,"链接":"https://arxiv.org/abs/2504.13201","备注/讨论点":null,"父记录":null},{"标题":"DoRO: Disambiguation of Referred Object for Embodied Agents","文章简称":"DoRO","\\cite{}":"pramanick2022doro","发表年月":22.1,"10大原则":"可信-准确","4个阶段":"指令理解","一作":"Pradip Pramanick","一作单位":"TCS Research","通讯":"Chayan Sarkar","通讯单位":"TCS Research","具身任务":"Grounding (2D)","方法论":"小模型","发表情况":"IEEE Robotics and Automation Letters, 2022","链接":"https://ieeexplore.ieee.org/abstract/document/9846930","备注/讨论点":"关键词：模糊指令","父记录":null},{"标题":"Embodied Instruction Following in Unknown Environments","文章简称":"Wu et al.","\\cite{}":"wu2025embodiedinstructionfollowingunknown","发表年月":25.07,"10大原则":"可信-可靠","4个阶段":"指令理解","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://www.arxiv.org/pdf/2406.11818","备注/讨论点":null,"父记录":null},{"标题":"Embodied Multi-Agent Task Planning from Ambiguous Instruction","文章简称":"Liu et al.","\\cite{}":"liu2022embodied","发表年月":22.06,"10大原则":"可信-准确","4个阶段":"指令理解","一作":"Xinzhu Liu","一作单位":"THU","通讯":"Huaping Liu","通讯单位":"THU","具身任务":"Planning","方法论":"Benchmark, 小模型","发表情况":"RSS 22","链接":"https://web.archive.org/web/20220704170254id_/http://www.roboticsproceedings.org/rss18/p032.pdf","备注/讨论点":"关键词：模糊指令、多智能体","父记录":null},{"标题":"Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning","文章简称":"Ramrakhya et al.","\\cite{}":"ramrakhya2025grounding","发表年月":25.04,"10大原则":"可信-准确","4个阶段":"指令理解","一作":"Ram Ramrakhya","一作单位":"Georgia Institute of Technology, Meta","通讯":null,"通讯单位":null,"具身任务":"EQA(2D)","方法论":"Benchmark, MLLM, 强化学习","发表情况":null,"链接":"https://arxiv.org/abs/2504.00907","备注/讨论点":"关键词：模糊指令/指令歧义","父记录":null},{"标题":"Improving Grounded Natural Language Understanding through Human-Robot Dialog","文章简称":"Thomason et al.","\\cite{}":"thomason2019improving","发表年月":19.05,"10大原则":"可信-准确","4个阶段":"指令理解","一作":"Jesse Thomason","一作单位":"University of Washington","通讯":null,"通讯单位":null,"具身任务":"Navigation, delivery, relocation","方法论":"迭代式交互","发表情况":"ICRA 19","链接":"https://ieeexplore.ieee.org/abstract/document/8794287","备注/讨论点":"关键词：指令对话","父记录":null},{"标题":"Inner Monologue: Embodied Reasoning through Planning with Language Models","文章简称":"Inner Monologue","\\cite{}":"huang2022innermonologueembodiedreasoning","发表年月":22.07,"10大原则":"可信-准确","4个阶段":"指令理解","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2207.05608","备注/讨论点":null,"父记录":null},{"标题":"Integrating Disambiguation and User Preferences into Large Language Models for Robot Motion Planning","文章简称":"Abugurain et al.","\\cite{}":"abugurain2024integrating","发表年月":24.04,"10大原则":"可信-准确","4个阶段":"指令理解","一作":"Mohammed Abugurain","一作单位":"KAUST","通讯":null,"通讯单位":null,"具身任务":"Navigation","方法论":"LLM","发表情况":null,"链接":"https://arxiv.org/abs/2404.14547","备注/讨论点":"关键词：指令歧义、用户偏好","父记录":null},{"标题":"Jailbreaking LLM-Controlled Robots","文章简称":"Robey et al.","\\cite{}":"robey2024jailbreakingllmcontrolledrobots","发表年月":24.1,"10大原则":"安全-抗攻击","4个阶段":"指令理解","一作":"Alexander Robey","一作单位":"UPenn","通讯":null,"通讯单位":null,"具身任务":"EQA(2D)","方法论":"Benchmark, Prompt","发表情况":"ICRA 2025","链接":"https://arxiv.org/abs/2410.13691","备注/讨论点":null,"父记录":null},{"标题":"LACMA: Language-Aligning Contrastive Learning with Meta-Actions for Embodied Instruction Following","文章简称":"LACMA","\\cite{}":"yang2023lacmalanguagealigningcontrastivelearning","发表年月":23.1,"10大原则":"可信-可靠","4个阶段":"指令理解","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2310.12344","备注/讨论点":null,"父记录":null},{"标题":"LLM-Driven Robots Risk Enacting Discrimination, Violence, and Unlawful Actions","文章简称":"Azeem et al.","\\cite{}":"azeem_llm-driven_2024","发表年月":24.06,"10大原则":"安全-价值对齐","4个阶段":"指令理解","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2406.08824","备注/讨论点":null,"父记录":null},{"标题":"MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models","文章简称":"MM-SafetyBench","\\cite{}":"liu2024mmsafetybenchbenchmarksafetyevaluation","发表年月":23.11,"10大原则":"安全-抗攻击","4个阶段":"指令理解","一作":"Xin Liu","一作单位":"PJLAB, ECNU","通讯":"Yunshi Lan, Chao Yang","通讯单位":"PJLAB, ECNU","具身任务":"EQA(2D)","方法论":"Benchmark, MLLM","发表情况":"ECCV 2024","链接":"https://arxiv.org/abs/2311.17600","备注/讨论点":null,"父记录":null},{"标题":"NarraGuide: an LLM-based Narrative Mobile Robot for Remote Place Exploration","文章简称":"NarraGuide","\\cite{}":"hu2025narraguide","发表年月":25.08,"10大原则":"可信-准确","4个阶段":"指令理解","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://www.arxiv.org/abs/2508.01235","备注/讨论点":null,"父记录":null},{"标题":"Navigation as Attackers Wish? Towards Building Robust Embodied Agents under Federated Learning","文章简称":"NoisyEQA","\\cite{}":"wu2024noisyeqabenchmarkingembodiedquestion","发表年月":22.11,"10大原则":"可信-准确","4个阶段":"指令理解","一作":"Yunchao Zhang","一作单位":"UCSC","通讯":null,"通讯单位":null,"具身任务":"VLN","方法论":"指令微调, MLLM","发表情况":"ACL 2024","链接":"https://arxiv.org/abs/2211.14769","备注/讨论点":null,"父记录":null},{"标题":"Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models","文章简称":"HELPER","\\cite{}":"sarch2023openendedinstructableembodiedagents","发表年月":23.1,"10大原则":"可信-准确","4个阶段":"指令理解","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2310.15127","备注/讨论点":null,"父记录":null},{"标题":"POEX: Understanding and Mitigating Policy Executable Jailbreak Attacks against Embodied AI","文章简称":"POEX","\\cite{}":"lu2025poexunderstandingmitigatingpolicy","发表年月":24.12,"10大原则":"安全-抗攻击","4个阶段":"指令理解","一作":"Xuancun Lu","一作单位":"ZJU","通讯":null,"通讯单位":null,"具身任务":"Planning, EQA(2D)","方法论":"MLLM, Benchmark, Prompt","发表情况":null,"链接":"https://arxiv.org/abs/2412.16633","备注/讨论点":null,"父记录":null},{"标题":"Plug in the Safety Chip: Enforcing Constraints for LLM-driven Robot Agents","文章简称":"Yang et al.","\\cite{}":"yang2023plugsafetychipenforcing","发表年月":23.09,"10大原则":"安全-防滥用","4个阶段":"指令理解","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2309.09919","备注/讨论点":null,"父记录":null},{"标题":"Security Considerations in AI-Robotics: A Survey of Current Methods, Challenges, and Opportunities","文章简称":"Neupane et al.","\\cite{}":"neupane2024securityconsiderationsairoboticssurvey","发表年月":23.1,"10大原则":"安全-隐私可保护","4个阶段":"指令理解","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2310.08565","备注/讨论点":null,"父记录":null},{"标题":"Semantic Skill Grounding for Embodied Instruction-Following in Cross-Domain Environments","文章简称":"SemGro","\\cite{}":"shin2024semanticskillgroundingembodied","发表年月":24.08,"10大原则":"可信-可靠","4个阶段":"指令理解","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2408.01024","备注/讨论点":null,"父记录":null},{"标题":"ThinkBot: Embodied Instruction Following with Thought Chain Reasoning","文章简称":"ThinkBot","\\cite{}":"lu2023thinkbotembodiedinstructionfollowing","发表年月":23.12,"10大原则":"可信-准确","4个阶段":"指令理解","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2312.07062","备注/讨论点":null,"父记录":null},{"标题":"Towards Robust Multimodal Large Language Models Against Jailbreak Attacks","文章简称":"SAFEMLLM","\\cite{}":"yin2025robustmultimodallargelanguage","发表年月":25.02,"10大原则":"安全-抗攻击","4个阶段":"指令理解","一作":"Ziyi Yin","一作单位":"UPenn","通讯":null,"通讯单位":null,"具身任务":"EQA(2D)","方法论":"adversarial attack, MLLM","发表情况":null,"链接":"https://arxiv.org/abs/2502.00653","备注/讨论点":null,"父记录":null},{"标题":"Verifiably Following Complex Robot Instructions with Foundation Models","文章简称":"LIMP","\\cite{}":"quartey2025verifiablyfollowingcomplexrobot","发表年月":24.02,"10大原则":"可信-可靠","4个阶段":"指令理解","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2402.11498","备注/讨论点":null,"父记录":null},{"标题":"tagE: Enabling an Embodied Agent to Understand Human Instructions","文章简称":"tagE","\\cite{}":"sarkar2023tageenablingembodiedagent","发表年月":23.1,"10大原则":"可信-准确","4个阶段":"指令理解","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2310.15605","备注/讨论点":null,"父记录":null},{"标题":"A Survey on Adversarial Robustness of LiDAR-based Machine Learning Perception in Autonomous Vehicles","文章简称":"Kim et al.","\\cite{}":"kim_survey_2024","发表年月":24.11,"10大原则":"安全-抗攻击, 安全-防滥用","4个阶段":"环境感知","一作":"Junae Kim","一作单位":null,"通讯":null,"通讯单位":null,"具身任务":"survey","方法论":"survey","发表情况":null,"链接":"https://arxiv.org/pdf/2411.13778v1","备注/讨论点":"环境感知","父记录":null},{"标题":"Active SLAM With Dynamic Viewpoint Optimization for Robust Visual Navigation","文章简称":"Li et al.","\\cite{}":"li2025active","发表年月":25.06,"10大原则":"可信-准确, 可信-可靠","4个阶段":"环境感知","一作":"Peng Li","一作单位":"Chinese Academy of Sciences","通讯":"Zhengxing Wu","通讯单位":"University of Chinese Academy of Sciences","具身任务":"Navigation","方法论":"SLAM","发表情况":"IEEE Transactions on Instrumentation and Measurement 2025","链接":"https://ieeexplore.ieee.org/abstract/document/11037221","备注/讨论点":null,"父记录":null},{"标题":"Adversarial Attacks and Detection in Visual Place Recognition for Safer Robot Navigation","文章简称":"Malone et al.","\\cite{}":"malone_adversarial_2025","发表年月":25.01,"10大原则":"安全-抗攻击, 安全-防滥用","4个阶段":"环境感知","一作":"Connor Malone","一作单位":"Queensland University of Technology","通讯":"Michael Milford","通讯单位":"Queensland University of Technology","具身任务":"Visual Place Recognition","方法论":"adversarial attack, Defence","发表情况":"IROS 2025","链接":"https://arxiv.org/pdf/2506.15988","备注/讨论点":"环境感知","父记录":null},{"标题":"An Enactive Approach to Value Alignment in Artificial Intelligence: A Matter of Relevance","文章简称":"Cannon et al.","\\cite{}":"cannon2021enactive","发表年月":25.11,"10大原则":"安全-价值对齐","4个阶段":"环境感知","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://philpapers.org/archive/CANAEA-5.pdf","备注/讨论点":null,"父记录":null},{"标题":"AuditMAI: Towards An Infrastructure for Continuous AI Auditing","文章简称":"AuditMAI","\\cite{}":"waltersdorfer2024auditmai","发表年月":24.06,"10大原则":"可信-可审计","4个阶段":"环境感知","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2406.14243","备注/讨论点":null,"父记录":null},{"标题":"BadDepth: Backdoor Attacks Against Monocular Depth Estimation in the Physical World","文章简称":"BadDepth","\\cite{}":"guo_baddepth_2025","发表年月":25.05,"10大原则":"安全-抗攻击, 安全-防滥用","4个阶段":"环境感知","一作":"Ji Guo","一作单位":"USTC","通讯":"Jiaming He","通讯单位":"USTC","具身任务":"SLAM","方法论":"adversarial attack","发表情况":null,"链接":"https://arxiv.org/pdf/2505.16154","备注/讨论点":"环境感知","父记录":null},{"标题":"E2CL: exploration-based error correction learning for embodied agents","文章简称":"E2CL","\\cite{}":"wang2024e2cl","发表年月":24.09,"10大原则":"可信-可解释","4个阶段":"环境感知","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2409.03256","备注/讨论点":null,"父记录":null},{"标题":"Embodied Active Defense: Leveraging Recurrent Feedback to Counter Adversarial Patches","文章简称":"EAD","\\cite{}":"wu2024embodied","发表年月":24.03,"10大原则":"安全-抗攻击, 安全-防滥用","4个阶段":"环境感知","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2404.00540","备注/讨论点":null,"父记录":null},{"标题":"Embodied Laser Attack:Leveraging Scene Priors to Achieve Agent-based Robust Non-contact Attacks","文章简称":"ELA","\\cite{}":"sun2024embodied","发表年月":24.07,"10大原则":"安全-抗攻击, 安全-防滥用","4个阶段":"环境感知","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2312.09554","备注/讨论点":null,"父记录":null},{"标题":"Embodied Uncertainty-Aware Object Segmentation","文章简称":"UNCOS","\\cite{}":"fang2024embodied","发表年月":24.1,"10大原则":"可信-准确","4个阶段":"环境感知","一作":"fang2024embodied","一作单位":"MIT","通讯":null,"通讯单位":null,"具身任务":"Segmentation","方法论":"不确定度","发表情况":"IROS 2024","链接":"https://ieeexplore.ieee.org/abstract/document/10801562","备注/讨论点":null,"父记录":null},{"标题":"Embodied active domain adaptation for semantic segmentation via informative path planning","文章简称":'Zurbr{\\"u}gg et al.',"\\cite{}":"zurbrugg2022embodied","发表年月":22.1,"10大原则":"可信-准确","4个阶段":"环境感知","一作":"René Zurbrügg","一作单位":"ETH Zurich","通讯":"René Zurbrügg","通讯单位":"ETH Zurich","具身任务":"Segmentation","方法论":"Domain Adaptation","发表情况":"IEEE Robotics and Automation Letters 2022","链接":"https://ieeexplore.ieee.org/abstract/document/9816133","备注/讨论点":null,"父记录":null},{"标题":"Embodied videoagent: Persistent memory from egocentric videos and embodied sensors enables dynamic scene understanding","文章简称":"Embodied VideoAgent","\\cite{}":"fan2024embodied","发表年月":25.01,"10大原则":"可信-可解释","4个阶段":"环境感知","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2501.00358","备注/讨论点":null,"父记录":null},{"标题":"Embodied visual active learning for semantic segmentation","文章简称":"Nilsson et al.","\\cite{}":"nilsson2021embodied","发表年月":21.12,"10大原则":"可信-准确","4个阶段":"环境感知","一作":"David Nilsson","一作单位":"Lund University, Google","通讯":null,"通讯单位":null,"具身任务":"Segmentation","方法论":"Active Learning, Self-supervised","发表情况":"AAAI 2021","链接":"https://ojs.aaai.org/index.php/AAAI/article/view/16338","备注/讨论点":null,"父记录":null},{"标题":"Embodiedgpt: Vision-language pre-training via embodied chain of thought","文章简称":"EmbodiedGPT","\\cite{}":"mu2023embodiedgpt","发表年月":23.05,"10大原则":"可信-准确","4个阶段":"环境感知","一作":"Yao Mu","一作单位":"HKU","通讯":"Ping Luo","通讯单位":"HKU","具身任务":"EQA(2D), Caption","方法论":"LLM","发表情况":"NeurIPS 2023","链接":"https://proceedings.neurips.cc/paper_files/paper/2023/file/4ec43957eda1126ad4887995d05fae3b-Paper-Conference.pdf","备注/讨论点":null,"父记录":null},{"标题":"Embodiedscan: A holistic multi-modal 3d perception suite towards embodied ai","文章简称":"EmbodiedScan","\\cite{}":"wang2024embodiedscan","发表年月":24.03,"10大原则":"可信-准确","4个阶段":"环境感知","一作":"Tai Wang","一作单位":"Shanghai AI Laborator","通讯":"Jiangmiao Pang","通讯单位":"Shanghai AI Laborator","具身任务":"Grounding (3D)","方法论":"Benchmark","发表情况":"CVPR 2024","链接":"https://openaccess.thecvf.com/content/CVPR2024/html/Wang_EmbodiedScan_A_Holistic_Multi-Modal_3D_Perception_Suite_Towards_Embodied_AI_CVPR_2024_paper.html","备注/讨论点":null,"父记录":null},{"标题":"Enhancing embodied object detection through language-image pre-training and implicit object memory","文章简称":"Chapman et al.","\\cite{}":"chapman2024enhancing","发表年月":24.02,"10大原则":"可信-准确","4个阶段":"环境感知","一作":"Nicolas Harvey Chapman","一作单位":null,"通讯":null,"通讯单位":null,"具身任务":"Object Detection","方法论":null,"发表情况":"arxiv 2024","链接":"https://arxiv.org/pdf/2402.03721","备注/讨论点":null,"父记录":null},{"标题":"From Strangers to Assistants: Fast Desire Alignment for Embodied Agent-User Adaptation","文章简称":"Wang et al.","\\cite{}":"wang2025strangers","发表年月":25.05,"10大原则":"安全-价值对齐","4个阶段":"环境感知","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2505.22503","备注/讨论点":null,"父记录":null},{"标题":"Good time to ask: A learning framework for asking for help in embodied visual navigation","文章简称":"GTA","\\cite{}":"zhang2023good","发表年月":23.06,"10大原则":"可信-可解释","4个阶段":"环境感知","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2206.10606","备注/讨论点":null,"父记录":null},{"标题":"Improved Semantic Segmentation from Ultra-Low-Resolution RGB Images Applied to Privacy-Preserving Object-Goal Navigation","文章简称":"Huang et al.","\\cite{}":"huang2025improved","发表年月":25.07,"10大原则":"安全-隐私可保护","4个阶段":"环境感知","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2507.16034","备注/讨论点":null,"父记录":null},{"标题":"Interactive task learning via embodied corrective feedback","文章简称":"Appelgren et al.","\\cite{}":"appelgren2020interactive","发表年月":20.09,"10大原则":"可信-可解释","4个阶段":"环境感知","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://link.springer.com/article/10.1007/s10458-020-09481-8","备注/讨论点":null,"父记录":null},{"标题":"Interactron: Embodied adaptive object detection","文章简称":"INTERACTRON","\\cite{}":"kotar2022interactron","发表年月":22.03,"10大原则":"可信-准确","4个阶段":"环境感知","一作":"Klemen Kotar","一作单位":"Allen Institute for AI","通讯":null,"通讯单位":null,"具身任务":"Object Detection","方法论":"Self-supervised, Active Learning","发表情况":"CVPR 2022","链接":"https://openaccess.thecvf.com/content/CVPR2022/papers/Kotar_Interactron_Embodied_Adaptive_Object_Detection_CVPR_2022_paper.pdf","备注/讨论点":null,"父记录":null},{"标题":"Is the robot spying on me? a study on perceived privacy in telepresence scenarios in a care setting with mobile and humanoid robots","文章简称":"Nieto et al.","\\cite{}":"nieto2024robot","发表年月":24.08,"10大原则":"安全-隐私可保护","4个阶段":"环境感知","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://link.springer.com/article/10.1007/s12369-024-01153-x","备注/讨论点":null,"父记录":null},{"标题":"Learn how to see: collaborative embodied learning for object detection and camera adjusting","文章简称":"STF","\\cite{}":"shen2024learn","发表年月":24.03,"10大原则":"可信-准确","4个阶段":"环境感知","一作":"Lingdong Shen","一作单位":"University of Chinese Academy of Sciences","通讯":"Chunlei Huo","通讯单位":"University of Chinese Academy of Sciences","具身任务":"Object Detection","方法论":null,"发表情况":"AAAI 2024","链接":"https://ojs.aaai.org/index.php/AAAI/article/view/28281","备注/讨论点":null,"父记录":null},{"标题":"Learning robust perceptive locomotion for quadrupedal robots in the wild","文章简称":"Miki et al.","\\cite{}":"miki2022learning","发表年月":22.01,"10大原则":"可信-准确, 可信-可靠","4个阶段":"环境感知","一作":"TAKAHIRO MIKI","一作单位":"ETH Zurich","通讯":null,"通讯单位":null,"具身任务":"Locomotion","方法论":"强化学习","发表情况":"Science Robotics 2022","链接":"https://arxiv.org/abs/2201.08117","备注/讨论点":"关键词：传感器外部感知；需要讨论是否属于范畴","父记录":null},{"标题":"Learning to Walk by Steering: Perceptive Quadrupedal Locomotion in Dynamic Environments","文章简称":"PRELUDE","\\cite{}":"seo2023learning","发表年月":22.09,"10大原则":"可信-准确, 可信-可靠","4个阶段":"环境感知","一作":"Mingyo Seo","一作单位":"The University of Texas at Austin","通讯":null,"通讯单位":null,"具身任务":"Locomotion","方法论":"强化学习","发表情况":"arxiv 2022","链接":"https://arxiv.org/abs/2209.09233","备注/讨论点":null,"父记录":null},{"标题":"Legged locomotion in challenging terrains using egocentric vision","文章简称":"Agarwal et al.","\\cite{}":"agarwal2023legged","发表年月":22.11,"10大原则":"可信-准确, 可信-可靠","4个阶段":"环境感知","一作":"Ananye Agarwal","一作单位":"Carnegie Mellon University","通讯":"Deepak Pathak","通讯单位":"Carnegie Mellon University","具身任务":"Locomotion","方法论":"强化学习","发表情况":"CoRL 2023","链接":"https://proceedings.mlr.press/v205/agarwal23a.html","备注/讨论点":null,"父记录":null},{"标题":"Monitoring and Diagnosability of Perception Systems","文章简称":"Antonante et al.","\\cite{}":"antonante2021monitoring","发表年月":20.11,"10大原则":"可信-可审计","4个阶段":"环境感知","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2005.11816","备注/讨论点":null,"父记录":null},{"标题":"Move to see better: Self-improving embodied object detection","文章简称":"Fang et al. ","\\cite{}":"fang2020move","发表年月":20.12,"10大原则":"可信-准确","4个阶段":"环境感知","一作":"Zhaoyuan Fang","一作单位":"Carnegie Mellon University","通讯":null,"通讯单位":null,"具身任务":"Object Detection","方法论":"Active Learning","发表情况":"bmvc2021","链接":"https://www.bmvc2021-virtualconference.com/assets/papers/0615.pdf","备注/讨论点":null,"父记录":null},{"标题":"Obstacle-Aware Quadrupedal Locomotion With Resilient Multi-Modal Reinforcement Learning","文章简称":"Nahrendra et al.","\\cite{}":"nahrendra2024obstacle","发表年月":24.09,"10大原则":"可信-准确, 可信-可靠","4个阶段":"环境感知","一作":"I Made Aswin Nahrendra","一作单位":"KAIST","通讯":"Hyun Myung","通讯单位":"KAIST","具身任务":"Locomotion","方法论":"强化学习","发表情况":"arxiv 2024","链接":"https://arxiv.org/abs/2409.19709","备注/讨论点":null,"父记录":null},{"标题":"On the Sensory Commutativity of Action Sequences for Embodied Agents","文章简称":"SCP","\\cite{}":"caselles2020sensory","发表年月":21.01,"10大原则":"安全-价值对齐","4个阶段":"环境感知","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2002.05630","备注/讨论点":null,"父记录":null},{"标题":"Openvla: An open-source vision-language-action model","文章简称":"OpenVLA","\\cite{}":"kim2024openvla","发表年月":24.06,"10大原则":"可信-准确","4个阶段":"环境感知","一作":"Moo Jin Kim","一作单位":"Stanford University","通讯":null,"通讯单位":null,"具身任务":"VLA","方法论":"VLA","发表情况":"arxiv 2024","链接":"https://arxiv.org/abs/2406.09246","备注/讨论点":null,"父记录":null},{"标题":"Palm-e: An embodied multimodal language model","文章简称":"PALM-E","\\cite{}":"driess2023palm","发表年月":23.07,"10大原则":"可信-准确","4个阶段":"环境感知","一作":"Danny Driess","一作单位":"Google, TU Berlin","通讯":null,"通讯单位":null,"具身任务":"EQA(2D), Manipulation, Caption","方法论":"MLLM","发表情况":"ICML 2023","链接":"https://dl.acm.org/doi/abs/10.5555/3618408.3618748","备注/讨论点":null,"父记录":null},{"标题":"Perception Matters: Enhancing Embodied AI with Uncertainty-Aware Semantic Segmentation","文章简称":"Prasanna et al.","\\cite{}":"prasanna2024perception","发表年月":24.08,"10大原则":"可信-准确","4个阶段":"环境感知","一作":"Sai Prasanna","一作单位":"University of Freiburg","通讯":null,"通讯单位":null,"具身任务":"Object Search, Segmentation","方法论":"不确定度","发表情况":"arxiv 2024","链接":"https://arxiv.org/pdf/2408.02297","备注/讨论点":null,"父记录":null},{"标题":"Privacy Risks of Robot Vision: A User Study on Image Modalities and Resolution","文章简称":"Huang et al.","\\cite{}":"huang2025privacy","发表年月":25.05,"10大原则":"安全-隐私可保护","4个阶段":"环境感知","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2505.07766","备注/讨论点":null,"父记录":null},{"标题":"Privacy beyond Data: Assessment and Mitigation of Privacy Risks in Robotic Technology for Elderly Care","文章简称":"Grabler et al.","\\cite{}":"grabler2025privacy","发表年月":24.11,"10大原则":"安全-隐私可保护","4个阶段":"环境感知","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://dl.acm.org/doi/10.1145/3689216","备注/讨论点":null,"父记录":null},{"标题":"Privacy-preserving robot vision with anonymized faces by extreme low resolution","文章简称":"Kim et al.","\\cite{}":"kim2019privacy","发表年月":19.11,"10大原则":"安全-隐私可保护","4个阶段":"环境感知","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://ieeexplore.ieee.org/document/8967681","备注/讨论点":null,"父记录":null},{"标题":"RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control","文章简称":"RT","\\cite{}":"pmlr-v229-zitkovich23a","发表年月":23.07,"10大原则":"可信-准确","4个阶段":"环境感知","一作":null,"一作单位":"Google DeepMind","通讯":null,"通讯单位":null,"具身任务":"VLA","方法论":"VLA","发表情况":"CoRL 2023","链接":"https://proceedings.mlr.press/v229/zitkovich23a.html","备注/讨论点":null,"父记录":null},{"标题":"Random Spoofing Attack against Scan Matching Algorithm SLAM (Long)","文章简称":"Fukunaga et al.","\\cite{}":"noauthor_random_nodate","发表年月":24.02,"10大原则":"安全-抗攻击, 安全-防滥用","4个阶段":"环境感知","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://www.ndss-symposium.org/ndss-paper/auto-draft-476/","备注/讨论点":null,"父记录":null},{"标题":"Real-time privacy preservation for robot visual perception","文章简称":"PCVS","\\cite{}":"choi2025real","发表年月":25.05,"10大原则":"安全-隐私可保护","4个阶段":"环境感知","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2505.05519","备注/讨论点":null,"父记录":null},{"标题":"Resilient Legged Local Navigation: Learning to Traverse with Compromised Perception End-to-End","文章简称":"Zhang et al.","\\cite{}":"zhang2024resilient","发表年月":23.1,"10大原则":"可信-准确, 可信-可靠","4个阶段":"环境感知","一作":"Jin Jin","一作单位":"ETH Zurich","通讯":null,"通讯单位":null,"具身任务":"Navigation","方法论":"强化学习","发表情况":"ICRA 24","链接":"https://ieeexplore.ieee.org/abstract/document/10611254","备注/讨论点":"关键词：感知失效","父记录":null},{"标题":"Robot Manipulation Based on Embodied Visual Perception: A Survey","文章简称":"Wang et al.","\\cite{}":"wang2025robot","发表年月":25.06,"10大原则":"可信-准确","4个阶段":"环境感知","一作":"Sicheng Wang","一作单位":"The Shenzhen Institute of Artificial Intelligence and Robotics for Societ","通讯":"Tianwei Zhang","通讯单位":"The Shenzhen Institute of Artificial Intelligence and Robotics for Society","具身任务":"survey","方法论":"survey","发表情况":"CAAI Transactions on Intelligence Technology 2025","链接":"https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/cit2.70022","备注/讨论点":null,"父记录":null},{"标题":"Robustnav: Towards benchmarking robustness in embodied navigation","文章简称":"RobustNAV","\\cite{}":"chattopadhyay2021robustnav","发表年月":21.06,"10大原则":"可信-准确, 可信-可靠","4个阶段":"环境感知","一作":"Prithvijit Chattopadhyay","一作单位":"Georgia Tech","通讯":null,"通讯单位":null,"具身任务":"Navigation","方法论":"Benchmark","发表情况":"ICCV 2021","链接":"https://openaccess.thecvf.com/content/ICCV2021/html/Chattopadhyay_RobustNav_Towards_Benchmarking_Robustness_in_Embodied_Navigation_ICCV_2021_paper.html","备注/讨论点":null,"父记录":null},{"标题":"Robustness of embodied point navigation agents","文章简称":"Frano Rajiˇc","\\cite{}":"rajivc2022robustness","发表年月":23.02,"10大原则":"可信-准确, 可信-可靠","4个阶段":"环境感知","一作":"Frano Rajiˇc","一作单位":"Swiss Federal Institute of Technology Lausanne","通讯":"Frano Rajiˇc","通讯单位":"Swiss Federal Institute of Technology Lausanne","具身任务":"Navigation","方法论":null,"发表情况":"ECCV 2022","链接":"https://link.springer.com/chapter/10.1007/978-3-031-25075-0_15","备注/讨论点":null,"父记录":null},{"标题":"SLAMSpoof: Practical LiDAR Spoofing Attacks on Localization Systems Guided by Scan Matching Vulnerability Analysis","文章简称":"SLAMSpoof","\\cite{}":"noauthor_slamspoof_nodate","发表年月":25.02,"10大原则":"安全-抗攻击, 安全-防滥用","4个阶段":"环境感知","一作":"Rokuto Nagata","一作单位":"Amano Institute of Technology","通讯":"Kentaro Yoshioka","通讯单位":"Amano Institute of Technology","具身任务":"Auto Drive, Navigation","方法论":"Point-Wise SMVS, Frame-Wise SMVS","发表情况":null,"链接":"https://arxiv.org/html/2502.13641v1","备注/讨论点":"环境感知","父记录":null},{"标题":"Safety Assessment for Autonomous Systems' Perception Capabilities","文章简称":"Molloy et al.","\\cite{}":"molloy2022safety","发表年月":22.08,"10大原则":"可信-可审计","4个阶段":"环境感知","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2208.08237","备注/讨论点":null,"父记录":null},{"标题":"Self-Explainable Affordance Learning with \nEmbodied Caption","文章简称":"SEA","\\cite{}":"zhang2024self","发表年月":24.04,"10大原则":"可信-可解释","4个阶段":"环境感知","一作":"Zhipeng Zhang","一作单位":"Northwestern Polytechnical University","通讯":null,"通讯单位":null,"具身任务":"Visual affordance learning","方法论":"Benchmark","发表情况":null,"链接":"https://arxiv.org/abs/2404.05603","备注/讨论点":null,"父记录":null},{"标题":"SoK: Rethinking Sensor Spoofing Attacks against Robotic Vehicles from a Systematic View","文章简称":"SoK","\\cite{}":"xu2023sok","发表年月":23.07,"10大原则":"安全-抗攻击, 安全-防滥用","4个阶段":"环境感知","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2205.04662","备注/讨论点":null,"父记录":null},{"标题":"Towards Embodied Agent Intent Explanation in Human-Robot Collaboration: ACT Error Analysis and Solution Conceptualization","文章简称":"CRIE","\\cite{}":"ergogo2025towards","发表年月":25.05,"10大原则":"可信-可解释","4个阶段":"环境感知","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://openreview.net/forum?id=gPqkW8V6Je","备注/讨论点":null,"父记录":null},{"标题":"Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and Attacks","文章简称":"Xing et al.","\\cite{}":"xing2025robustsecureembodiedai","发表年月":25.02,"10大原则":"安全-抗攻击, 安全-防滥用","4个阶段":"环境感知","一作":"WENPENG XING","一作单位":"ZJU","通讯":"MENG HAN","通讯单位":"ZJU","具身任务":"survey","方法论":"survey","发表情况":null,"链接":"https://arxiv.org/pdf/2502.13175","备注/讨论点":"物理交互","父记录":null},{"标题":"Viewinfer3d: 3d visual grounding based on embodied viewpoint inference","文章简称":"ViewInfer3D","\\cite{}":"geng2024viewinfer3d","发表年月":24.07,"10大原则":"可信-准确","4个阶段":"环境感知","一作":"Liang Geng","一作单位":"Beijing University of Posts and Telecommunications","通讯":"Jianqin Yin","通讯单位":"Beijing University of Posts and Telecommunications","具身任务":"Grounding (3D)","方法论":"LLM","发表情况":"IEEE Robotics and Automation Letters 2024","链接":"https://ieeexplore.ieee.org/abstract/document/10592798","备注/讨论点":null,"父记录":null},{"标题":"Vima: General robot manipulation with multimodal prompts","文章简称":"VIMA","\\cite{}":"jiang2022vima","发表年月":22.1,"10大原则":"可信-准确","4个阶段":"环境感知","一作":"Yunfan Jiang","一作单位":"Stanford University","通讯":"Yuke Zhu","通讯单位":"NVIDIA","具身任务":"Manipulation","方法论":"Benchmark","发表情况":"arxiv 2022","链接":"https://arxiv.org/abs/2210.03094","备注/讨论点":null,"父记录":null},{"标题":"What do navigation agents learn about their environment?","文章简称":"iSEE","\\cite{}":"dwivedi2022navigation","发表年月":22.06,"10大原则":"可信-可解释","4个阶段":"环境感知","一作":"Kshitij Dwivedi","一作单位":"Goethe University Frankfurt","通讯":null,"通讯单位":null,"具身任务":"VLN","方法论":"Visualization","发表情况":"CVPR'22","链接":"https://openaccess.thecvf.com/content/CVPR2022/html/Dwivedi_What_Do_Navigation_Agents_Learn_About_Their_Environment_CVPR_2022_paper.html","备注/讨论点":null,"父记录":null},{"标题":"Ask4Help: Learning to Leverage an Expert for Embodied Tasks","文章简称":"Ask4Help","\\cite{}":"singh2022ask4help","发表年月":22.11,"10大原则":"可信-可靠","4个阶段":"决策规划","一作":"Kunal Pratap Singh ","一作单位":"Allen Institute for AI","通讯":null,"通讯单位":null,"具身任务":"Navigation, Room Rearrangement","方法论":"强化学习","发表情况":"NeurIPS 22","链接":"https://arxiv.org/abs/2211.09960","备注/讨论点":"关键词：交互","父记录":null},{"标题":"BadRobot: Jailbreaking Embodied LLMs in the Physical World","文章简称":"BadRobot","\\cite{}":"zhang2025badrobotjailbreakingembodiedllms","发表年月":24.07,"10大原则":"安全-抗攻击","4个阶段":"决策规划, 指令理解","一作":"Hangtao Zhang","一作单位":"HUST","通讯":null,"通讯单位":null,"具身任务":"Planning, EQA(2D)","方法论":"Attack Paradigm","发表情况":"ICLR 2025","链接":"https://arxiv.org/abs/2407.20242","备注/讨论点":null,"父记录":null},{"标题":"BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization","文章简称":"BadVLA","\\cite{}":"zhou2025badvla","发表年月":25.05,"10大原则":"安全-抗攻击","4个阶段":"决策规划","一作":"Xueyang Zhou","一作单位":"HUST","通讯":"Lichao Sun","通讯单位":"Lehigh University","具身任务":"VLA","方法论":"adversarial attack","发表情况":null,"链接":"https://arxiv.org/pdf/2505.16640","备注/讨论点":"行为规划","父记录":null},{"标题":"Characterizing Physical Adversarial Attacks on Robot Motion Planners","文章简称":"Wu et al.","\\cite{}":"wu2024characterizing","发表年月":24.01,"10大原则":"安全-抗攻击","4个阶段":"决策规划","一作":"Wenxi Wu","一作单位":"KCL","通讯":"Wenxi Wu","通讯单位":"KCL","具身任务":"Planning","方法论":"物理干预, adversarial attack","发表情况":"ICRA 2024","链接":"https://kclpure.kcl.ac.uk/ws/portalfiles/portal/248844190/icra2024_motion_planning_attacks.pdf","备注/讨论点":"行为规划","父记录":null},{"标题":"Disability 4.0: bioethical considerations on the use of embodied artificial intelligence\n\n","文章简称":"Disability 4.0","\\cite{}":"de2024disability","发表年月":24.08,"10大原则":"安全-价值对齐","4个阶段":"决策规划","一作":"Francesco De Micco","一作单位":"University Campus Bio-Medico of Rome","通讯":"Roberto Scendoni","通讯单位":"University of Macerata,  Macerata","具身任务":"survey, Manipulation","方法论":"survey","发表情况":null,"链接":"https://www.frontiersin.org/journals/medicine/articles/10.3389/fmed.2024.1437280/full","备注/讨论点":null,"父记录":null},{"标题":" DynaMem: Online Dynamic\nSpatio-Semantic Memory for Open World Mobile\nManipulation","文章简称":"DynaMem","\\cite{}":"liuDynaMemOnlineDynamic2025","发表年月":25.05,"10大原则":"可信-准确","4个阶段":"决策规划","一作":"Peiqi Liu","一作单位":"NYU","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/pdf/2411.04999","备注/讨论点":null,"父记录":null},{"标题":"EHAZOP: A Proof of Concept Ethical Hazard Analysis of an Assistive Robot","文章简称":"EHAZOP","\\cite{}":"menon2024ehazop","发表年月":24.06,"10大原则":"可信-可控","4个阶段":"决策规划","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/pdf/2406.09239","备注/讨论点":null,"父记录":null},{"标题":"Ella: Embodied Social Agents with Lifelong Memory","文章简称":"Ella","\\cite{}":"zhangEllaEmbodiedSocial2025","发表年月":25.06,"10大原则":"可信-准确","4个阶段":"决策规划","一作":"Hongxin Zhang","一作单位":"University of Massachusetts Amherst","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/pdf/2506.24019","备注/讨论点":null,"父记录":null},{"标题":"Embodied Escaping: End-to-End Reinforcement Learning for Robot Navigation in Narrow Environment","文章简称":"Embodied Escaping","\\cite{}":"zheng2025embodied","发表年月":25.03,"10大原则":"可信-可靠","4个阶段":"决策规划","一作":"Han Zheng","一作单位":"SJTU","通讯":null,"通讯单位":null,"具身任务":"Navigation","方法论":"强化学习","发表情况":null,"链接":"https://arxiv.org/abs/2503.03208","备注/讨论点":"关键词：狭窄环境","父记录":null},{"标题":"Embodied-RAG: General Non-\nparametric Embodied Memory for Retrieval and\nGeneration","文章简称":"Embodied-RAG","\\cite{}":"xieEmbodiedRAGGeneralNonparametric2025","发表年月":25.01,"10大原则":"可信-准确","4个阶段":"决策规划","一作":"Quanting Xie","一作单位":"CMU","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/pdf/2409.18313","备注/讨论点":null,"父记录":null},{"标题":"Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models","文章简称":"Liu et al.","\\cite{}":"liu2024exploring","发表年月":24.05,"10大原则":"安全-抗攻击","4个阶段":"决策规划","一作":"Shuyuan Liu","一作单位":"ECNU","通讯":"Zhaoxia Yin","通讯单位":"ECNU","具身任务":"Planning","方法论":"adversarial attack, MLLM","发表情况":"ACM MM 2024","链接":"https://arxiv.org/abs/2405.19802","备注/讨论点":null,"父记录":null},{"标题":"From Screens to Scenes: A Survey of Embodied AI in Healthcare","文章简称":"Naik ","\\cite{}":"naik2022legal","发表年月":25.03,"10大原则":"可信-可审计","4个阶段":"决策规划","一作":"Yihao Liu","一作单位":"Central South University","通讯":", Jintai Chen","通讯单位":"HKUST(Guangzhou)","具身任务":"survey","方法论":"survey","发表情况":null,"链接":"https://arxiv.org/abs/2501.07468","备注/讨论点":null,"父记录":null},{"标题":"Generating Explanations for Embodied Action Decision from Visual Observation","文章简称":"Wang et al.","\\cite{}":"wang2023generating","发表年月":23.1,"10大原则":"可信-可解释","4个阶段":"决策规划","一作":"Xiaohan Wang","一作单位":"XJTU","通讯":null,"通讯单位":null,"具身任务":"Planning, VLN","方法论":"Benchmark","发表情况":"MM'23","链接":"https://dl.acm.org/doi/10.1145/3581783.3612351","备注/讨论点":null,"父记录":null},{"标题":"Generative Agents: Interactive Simulacra of Human Behavior","文章简称":"generative agents","\\cite{}":"parkGenerativeAgentsInteractive2023","发表年月":23.08,"10大原则":"可信-准确","4个阶段":"决策规划","一作":"Joon Sung Park","一作单位":"Stanford University","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/pdf/2304.03442","备注/讨论点":null,"父记录":null},{"标题":"Habitat-web: Learning embod-\nied object-search strategies from human demonstra-\ntions at scale","文章简称":"Habitat-web","\\cite{}":"ramrakhya2022habitat","发表年月":22.04,"10大原则":"可信-可靠","4个阶段":"决策规划","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2204.03514","备注/讨论点":null,"父记录":null},{"标题":"HomeRobot: Open-Vocabulary Mobile\nManipulation","文章简称":"HomeRobot: ","\\cite{}":"yenamandraHomeRobotOpenVocabularyMobile2024","发表年月":23.06,"10大原则":"可信-准确","4个阶段":"决策规划","一作":"Sriram Yenamandra","一作单位":"Georgia Tech","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/pdf/2306.11565","备注/讨论点":null,"父记录":null},{"标题":"Humanizing AI in medical training: ethical framework for responsible design","文章简称":"Tahri Sqalli et al.","\\cite{}":"tahri2023humanizing","发表年月":23.05,"10大原则":"安全-价值对齐","4个阶段":"决策规划","一作":"Mohammed Tahri Sqalli","一作单位":"GUQ","通讯":"Mohammed Tahri Sqalli","通讯单位":"GUQ","具身任务":"survey","方法论":"理论框架","发表情况":null,"链接":"https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2023.1189914/full","备注/讨论点":null,"父记录":null},{"标题":"INTRODUCING THE Robot Security Framework (RSF), A STANDARDIZED METHODOLOGY TO PERFORM SECURITY ASSESSMENTS IN ROBOTICS","文章简称":"RSF","\\cite{}":"adebayo2023introducing","发表年月":23.12,"10大原则":"可信-可审计","4个阶段":"决策规划","一作":"Abiodun Sunday Adebayo","一作单位":"University of Staffordshire","通讯":"Abiodun Sunday Adebayo","通讯单位":"University of Staffordshire","具身任务":"Manipulation","方法论":"混合模型, 理论框架","发表情况":"Journal of Frontiers in Multidisciplinary Research","链接":"https://www.multidisciplinaryfrontiers.com/uploads/archives/20250312183510_FMR-2025-1-004.1.pdf","备注/讨论点":null,"父记录":null},{"标题":"LLaPa: A Vision-Language Model Framework for Counterfactual-Aware Procedural Planning","文章简称":"LLaPa","\\cite{}":"sun2025llapa","发表年月":25.07,"10大原则":"可信-可靠","4个阶段":"决策规划","一作":"Shibo Sun","一作单位":"HIT","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/pdf/2507.08496","备注/讨论点":null,"父记录":null},{"标题":"Manipulating Neural Path Planners via Slight Perturbations","文章简称":"Xiong et al.","\\cite{}":"xiong2024manipulating","发表年月":24.03,"10大原则":"安全-隐私可保护","4个阶段":"决策规划","一作":"Zikang Xiong","一作单位":"Purdue University","通讯":" Suresh Jagannathan","通讯单位":"Purdue University","具身任务":"Navigation","方法论":"Backdoor Attack","发表情况":null,"链接":"https://arxiv.org/pdf/2403.18256","备注/讨论点":"行为规划","父记录":null},{"标题":"Modeling Dynamic Environments with Scene Graph Memory","文章简称":"Scene Graph\nMemory","\\cite{}":"kurenkovModelingDynamicEnvironments2023","发表年月":23.06,"10大原则":"可信-准确","4个阶段":"决策规划","一作":"Andrey Kurenkov","一作单位":"Stanford University","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/pdf/2305.17537","备注/讨论点":null,"父记录":null},{"标题":"Multi-Modal Multi-Task (M3T) Federated Foundation Models for Embodied AI: Potentials and Challenges for Edge Integration","文章简称":"Borazjani et al.","\\cite{}":"borazjani2025multi","发表年月":25.05,"10大原则":"安全-隐私可保护","4个阶段":"决策规划","一作":"Kasra Borazjani","一作单位":null,"通讯":null,"通讯单位":null,"具身任务":"survey","方法论":"联邦学习","发表情况":null,"链接":"https://arxiv.org/abs/2505.11191","备注/讨论点":null,"父记录":null},{"标题":"REACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS","文章简称":"REACT","\\cite{}":"yao2023react","发表年月":23.05,"10大原则":"可信-准确","4个阶段":"决策规划","一作":"Shunyu Yao","一作单位":"Princeton University","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":"ICLR2023","链接":"https://arxiv.org/pdf/2210.03629","备注/讨论点":null,"父记录":null},{"标题":"ReMEmbR: Building\nand Reasoning Over Long-Horizon Spatio-Temporal\nMemory for Robot Navigation","文章简称":"ReMEmbR","\\cite{}":"anwarReMEmbRBuildingReasoning2024","发表年月":24.09,"10大原则":"可信-准确","4个阶段":"决策规划","一作":"Abrar Anwar","一作单位":"NVIDIA, University of Southern California","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/pdf/2409.13682","备注/讨论点":null,"父记录":null},{"标题":"Reinforced Reasoning for Embodied Planning","文章简称":"Wu et al.","\\cite{}":"wu2025reinforced","发表年月":25.05,"10大原则":"可信-准确","4个阶段":"决策规划","一作":"Di Wu","一作单位":"SJTU","通讯":null,"通讯单位":null,"具身任务":"VLN","方法论":"强化学习, 指令微调, MLLM","发表情况":null,"链接":"https://arxiv.org/abs/2505.22050","备注/讨论点":"关键词：长程规划","父记录":null},{"标题":"Robo-Troj: Attacking LLM-based Task Planners","文章简称":"Robo-Troj","\\cite{}":"nahian2025robo","发表年月":25.04,"10大原则":"可信-准确","4个阶段":"决策规划","一作":"Mohaiminul Al Nahian; Zainab Altaweel","一作单位":"SUNY Binghamton","通讯":"Adnan Siraj Rakin","通讯单位":"SUNY Binghamton","具身任务":"Manipulation, Planning","方法论":"指令微调, LLM","发表情况":null,"链接":"https://arxiv.org/pdf/2504.17070","备注/讨论点":"行为规划","父记录":null},{"标题":"SEMI-PARAMETRIC TOPOLOGICAL MEMORY FOR NAVIGATION","文章简称":"SPTM","\\cite{}":"savinovSemiparametricTopologicalMemory2018","发表年月":18.05,"10大原则":"可信-准确","4个阶段":"决策规划","一作":"Nikolay Savinov","一作单位":"ETH Zurich","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/pdf/1803.00653","备注/讨论点":null,"父记录":null},{"标题":"SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents","文章简称":"SafeAgentBench","\\cite{}":"yin2024safeagentbench","发表年月":24.12,"10大原则":"可信-准确","4个阶段":"决策规划","一作":"Sheng Yin","一作单位":"SJTU","通讯":"Siheng Chen","通讯单位":"PJLAB","具身任务":"Planning","方法论":"LLM, Prompt, AI2-THOR, Benchmark","发表情况":null,"链接":"https://arxiv.org/abs/2412.13178","备注/讨论点":null,"父记录":null},{"标题":"Safety Aware Task Planning via Large Language Models in Robotics","文章简称":"SAFER","\\cite{}":"khan2025safety","发表年月":25.03,"10大原则":"可信-可控","4个阶段":"决策规划","一作":"Azal Ahmad Khan","一作单位":"University of Minnesota","通讯":null,"通讯单位":null,"具身任务":"Planning, Manipulation","方法论":"LLM, Prompt","发表情况":null,"链接":"https://arxiv.org/pdf/2503.15707","备注/讨论点":null,"父记录":null},{"标题":"Safety Control of Service Robots with LLMs and Embodied Knowledge Graphs","文章简称":"Qi et al. ","\\cite{}":"qi2024safety","发表年月":24.05,"10大原则":"可信-准确","4个阶段":"决策规划","一作":"Yong Qi","一作单位":"陕西科技大学","通讯":null,"通讯单位":null,"具身任务":"Manipulation, Planning","方法论":"LLM, Prompt, Knowledge Graph","发表情况":null,"链接":"https://arxiv.org/pdf/2405.17846","备注/讨论点":null,"父记录":null},{"标题":"Safety assurances for human-robot interaction via confidence-aware game-theoretic human models","文章简称":"Tian et al.","\\cite{}":"tian2022safety","发表年月":21.1,"10大原则":"可信-可控","4个阶段":"决策规划","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2109.14700","备注/讨论点":null,"父记录":null},{"标题":"SnapMem: Snapshot-based 3D Scene Memory for\nEmbodied Exploration and Reasoning","文章简称":"SnapMem","\\cite{}":"yangSnapMemSnapshotbased3D2024","发表年月":24.11,"10大原则":"可信-准确","4个阶段":"决策规划","一作":"Yuncong Yang","一作单位":"UMass Amherst","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/html/2411.17735v1","备注/讨论点":null,"父记录":null},{"标题":"Thinking in Space:\nHow Multimodal Large Language Models See, Re-\nmember, and Recall Spaces","文章简称":"Thinking in Space","\\cite{}":"yangThinkingSpaceHow2025","发表年月":24.12,"10大原则":"可信-准确","4个阶段":"决策规划","一作":"Jihan Yang","一作单位":"NYU","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/pdf/2412.14171","备注/讨论点":null,"父记录":null},{"标题":"Trust-aware motion planning for human-robot collaboration under distribution temporal logic specifications","文章简称":"Yu et al.","\\cite{}":"yu2024trust","发表年月":23.1,"10大原则":"可信-可控","4个阶段":"决策规划","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2310.01163","备注/讨论点":null,"父记录":null},{"标题":"Uncertainty in Action: Confidence Elicitation in Embodied Agents","文章简称":"Yu et al. ","\\cite{}":"yu2025uncertainty","发表年月":25.03,"10大原则":"可信-可靠","4个阶段":"决策规划","一作":"Tianjiao Yu","一作单位":"UIUC","通讯":null,"通讯单位":null,"具身任务":"Planning, VLN","方法论":"不确定度","发表情况":null,"链接":"https://arxiv.org/abs/2503.10628","备注/讨论点":"关键词：置信度","父记录":null},{"标题":"VLM-Social-Nav: Socially Aware Robot Navigation through Scoring using Vision-Language Models","文章简称":"VLM-Social-Nav","\\cite{}":"song2024vlm","发表年月":24.11,"10大原则":"可信-可控","4个阶段":"决策规划","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2404.00210","备注/讨论点":null,"父记录":null},{"标题":"Who’s in Charge Here? A Survey on Trustworthy AI in Variable Autonomy Robotic Systems","文章简称":"Methnani et al.","\\cite{}":"methnani2024s","发表年月":24.04,"10大原则":"可信-可控, 安全-价值对齐","4个阶段":"决策规划, 指令理解","一作":"Leila Methnani","一作单位":"UmU","通讯":null,"通讯单位":null,"具身任务":"survey, Auto Drive","方法论":"理论框架","发表情况":null,"链接":"https://dl.acm.org/doi/pdf/10.1145/3645090","备注/讨论点":null,"父记录":null},{"标题":"6-DOF Grasping for Target-driven Object Manipulation in Clutter","文章简称":" Collision-\nNet","\\cite{}":"murali20206","发表年月":19.12,"10大原则":"可信-可靠","4个阶段":"物理交互","一作":"Adithyavairavan Murali","一作单位":"NVIDIA","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/1912.03628","备注/讨论点":null,"父记录":null},{"标题":"A Review of Future and Ethical Perspectives of Robotics and AI","文章简称":"Torrensen（2018）","\\cite{}":"torresen2017review","发表年月":18.1,"10大原则":"安全-隐私可保护, 安全-价值对齐","4个阶段":"物理交互","一作":"Jim Torresen","一作单位":"University of Oslo","通讯":"Jim Torresen","通讯单位":"University of Oslo","具身任务":"survey, Manipulation","方法论":"survey, 理论框架","发表情况":null,"链接":"https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2017.00075/full","备注/讨论点":null,"父记录":null},{"标题":"A Secure Robot Learning Framework for Cyber Attack Scheduling and Countermeasure","文章简称":"Wu et al.","\\cite{}":"wu_secure_2023","发表年月":23.06,"10大原则":"安全-防滥用","4个阶段":"物理交互","一作":"Chengwei Wu","一作单位":"HIT","通讯":"Ligang Wu","通讯单位":"HIT","具身任务":null,"方法论":null,"发表情况":"TRO 2023","链接":"https://ieeexplore.ieee.org/document/10144090","备注/讨论点":"物理交互的攻防问题","父记录":null},{"标题":"AdvGrasp: Adversarial Attacks on Robotic Grasping from a Physical Perspective","文章简称":"AdvGrasp","\\cite{}":"wang_advgrasp_2025","发表年月":25.07,"10大原则":"安全-抗攻击","4个阶段":"物理交互","一作":"Xiaofei Wang","一作单位":"USTC","通讯":"Yunbo Zhao；Keke Tang","通讯单位":"USTC, Guangzhou University","具身任务":"Grasp","方法论":"adversarial attack, 模拟退火算法","发表情况":null,"链接":"https://arxiv.org/pdf/2507.09857","备注/讨论点":"物理交互","父记录":null},{"标题":"Contact-GraspNet: Efficient 6-DoF Grasp Generation in Cluttered Scenes","文章简称":"Contact-GraspNet","\\cite{}":"sundermeyer2021contact","发表年月":21.05,"10大原则":"可信-可靠","4个阶段":"物理交互","一作":"Martin Sundermeyer","一作单位":"NVIDIA","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2103.14127","备注/讨论点":null,"父记录":null},{"标题":"Controllability, Observability, Realizability, and Stability of Dynamic Linear Systems","文章简称":"Davis et al.","\\cite{}":"davis_controllability_2009","发表年月":9.01,"10大原则":"可信-可控","4个阶段":"物理交互","一作":"John M. Davis","一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/0901.3764","备注/讨论点":null,"父记录":null},{"标题":"Denoising diffusion probabilistic models","文章简称":"DDPM","\\cite{}":"ho2020denoising","发表年月":20.06,"10大原则":"可信-可靠","4个阶段":"物理交互","一作":"Jonathan Ho","一作单位":"UCB","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/pdf/2006.11239","备注/讨论点":null,"父记录":null},{"标题":"Dex-NeRF: Using a Neural Radiance Field to Grasp Transparent Objects","文章简称":"Dex-NeRF","\\cite{}":"ichnowski2022dex","发表年月":21.1,"10大原则":"可信-可靠","4个阶段":"物理交互","一作":"Jeffrey Ichnowski","一作单位":"UCB","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/pdf/2110.14217","备注/讨论点":null,"父记录":null},{"标题":"Diffusion Policy: Visuomotor Policy Learning via Action Diffusion","文章简称":"Diffusion Policy","\\cite{}":"chi2023diffusion","发表年月":23.03,"10大原则":"可信-可靠","4个阶段":"物理交互","一作":"Cheng Chi","一作单位":"Columbia University","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2303.04137","备注/讨论点":null,"父记录":null},{"标题":"Disability 4.0: bioethical considerations on the use of embodied artificial intelligence","文章简称":"Disability 4.0","\\cite{}":"demicco2024disability","发表年月":24.08,"10大原则":"安全-隐私可保护","4个阶段":"物理交互","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://www.frontiersin.org/journals/medicine/articles/10.3389/fmed.2024.1437280/full","备注/讨论点":null,"父记录":null},{"标题":"Form2Fit: Learning Shape Priors for Generalizable Assembly from Disassembly","文章简称":"Form2Fit","\\cite{}":"zakka2020form2fit","发表年月":19.1,"10大原则":"可信-可靠","4个阶段":"物理交互","一作":"Kevin Zakka","一作单位":"Stanford University","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/pdf/1910.13675","备注/讨论点":null,"父记录":null},{"标题":"From LLMs to Actions: Latent Codes as Bridges in Hierarchical Robot Control","文章简称":"LCB","\\cite{}":"shentu2024llmsactionslatentcodes","发表年月":24.05,"10大原则":"可信-可靠","4个阶段":"物理交互","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2405.04798","备注/讨论点":null,"父记录":null},{"标题":"Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation","文章简称":"HDP","\\cite{}":"ma2024hierarchical","发表年月":24.03,"10大原则":"可信-可靠","4个阶段":"物理交互","一作":"Xiao Ma","一作单位":"Dyson Robot Learning Lab","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/pdf/2403.03890","备注/讨论点":null,"父记录":null},{"标题":"Humanoid Robots in Tourism and Hospitality—Exploring Managerial, Ethical, and Societal Challenges","文章简称":"Skubis et al.","\\cite{}":"skubis2024humanoid","发表年月":24.12,"10大原则":"安全-隐私可保护, 安全-价值对齐","4个阶段":"物理交互","一作":"Ida Skubis","一作单位":"Silesian University of Technology","通讯":"Agata Mesjasz-Lech","通讯单位":"Czestochowa University of Technology","具身任务":"survey","方法论":"survey","发表情况":null,"链接":"https://www.mdpi.com/2076-3417/14/24/11823","备注/讨论点":null,"父记录":null},{"标题":"Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning","文章简称":"ViLa","\\cite{}":"hu2023look","发表年月":23.11,"10大原则":"可信-可靠","4个阶段":"物理交互","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2311.17842","备注/讨论点":null,"父记录":null},{"标题":"On the general theory of control systems","文章简称":"Kalman(1960)","\\cite{}":"kalman_general_1960","发表年月":1959.12,"10大原则":"可信-可控","4个阶段":"物理交互","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://www.sciencedirect.com/science/article/pii/S1474667017700948","备注/讨论点":null,"父记录":null},{"标题":"Optimal Actuator Attacks on Autonomous Vehicles Using Reinforcement Learning ","文章简称":"Wang et al.","\\cite{}":"wang_optimal_2025","发表年月":25.02,"10大原则":"安全-防滥用","4个阶段":"物理交互","一作":"Pengyu Wang","一作单位":"HKUST","通讯":"Ling Shi","通讯单位":"HKUST","具身任务":"Auto Drive","方法论":"强化学习","发表情况":null,"链接":"https://arxiv.org/pdf/2502.07839","备注/讨论点":"行为规划, 调整控制量，让注入的控制量能够躲过检测器的检测，又能够成功改变目标的轨迹","父记录":null},{"标题":"Partmanip: Learning cross-category generalizable part manipulation policy from point cloud observations","文章简称":"Partmanip","\\cite{}":"geng2023partmanip","发表年月":23.03,"10大原则":"可信-可靠","4个阶段":"物理交互","一作":"Haoran Geng","一作单位":"CFCS","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/pdf/2303.16958","备注/讨论点":null,"父记录":null},{"标题":"Planning with Diffusion for Flexible Behavior Synthesis","文章简称":"Diffuser","\\cite{}":"janner2022planning","发表年月":22.05,"10大原则":"可信-可靠","4个阶段":"物理交互","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2205.09991","备注/讨论点":null,"父记录":null},{"标题":"RIC: Rotate-Inpaint-Complete for Generalizable Scene Reconstruction","文章简称":"RIC","\\cite{}":"kasahara2023ric","发表年月":23.07,"10大原则":"可信-可靠","4个阶段":"物理交互","一作":"Isaac Kasahara","一作单位":"Samsung AI Center","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/pdf/2307.11932","备注/讨论点":null,"父记录":null},{"标题":"Robust Humanoid Locomotion Using Trajectory Optimization and Sample-Efficient Learning","文章简称":"Yeganegi et al.","\\cite{}":"yeganegi_robust_2019","发表年月":19.07,"10大原则":"安全-抗攻击","4个阶段":"物理交互","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/1907.04616","备注/讨论点":null,"父记录":null},{"标题":"Robust Push Recovery on Bipedal Robots: Leveraging Multi-Domain Hybrid Systems with Reduced-Order Model Predictive Control","文章简称":"Dai et al.","\\cite{}":"dai_robust_2025","发表年月":25.04,"10大原则":"安全-抗攻击","4个阶段":"物理交互","一作":"Min Dai","一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2504.18698","备注/讨论点":null,"父记录":null},{"标题":"Se (3)-diffusionfields: Learning smooth cost functions for joint grasp and motion optimization through diffusion","文章简称":"Se (3)-diffusionfields","\\cite{}":"urain2023se","发表年月":23.07,"10大原则":"可信-可靠","4个阶段":"物理交互","一作":"Julen Urain;Niklas Funk","一作单位":"Technische Universitat Darmstadt (Germany)","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161569","备注/讨论点":null,"父记录":null},{"标题":"SkillDiffuser: Interpretable Hierarchical Planning via Skill Abstractions in Diffusion-Based Task Execution","文章简称":"SkillDiffuser","\\cite{}":"liang2023skilldiffuser","发表年月":23.12,"10大原则":"可信-可靠","4个阶段":"物理交互","一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2312.11598","备注/讨论点":null,"父记录":null},{"标题":"TransCG: A Large-Scale Real-World Dataset for Transparent Object Depth Completion and a Grasping Baseline","文章简称":"Fang et al.","\\cite{}":"fang2022transcg","发表年月":22.02,"10大原则":"可信-可靠","4个阶段":"物理交互","一作":"Hongjie Fang","一作单位":"SJTU","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2202.08471","备注/讨论点":null,"父记录":null},{"标题":"Vision-Language-Action Model and Diffusion Policy Switching Enables Dexterous Control of an Anthropomorphic Hand","文章简称":null,"\\cite{}":"pan2024visionlanguageactionmodeldiffusionpolicy","发表年月":24.1,"10大原则":"可信-可靠","4个阶段":"物理交互","一作":"Cheng Pan","一作单位":"Swiss Federal Institute of Technology Lausanne","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2410.14022","备注/讨论点":null,"父记录":null},{"标题":"A Framework for Benchmarking and Aligning Task-Planning Safety in LLM-Based Embodied Agents","文章简称":null,"\\cite{}":null,"发表年月":25.04,"10大原则":null,"4个阶段":null,"一作":"Yuting Huang","一作单位":"USTC","通讯":"Mingxiao Ma; Yanyong Zhang","通讯单位":"USTC","具身任务":"Planning","方法论":"指令微调, LLM, VirtualHome, Benchmark","发表情况":null,"链接":"https://arxiv.org/abs/2504.14650","备注/讨论点":null,"父记录":null},{"标题":"A Survey of Embodied Learning for Object-Centric Robotic Manipulation ","文章简称":null,"\\cite{}":null,"发表年月":null,"10大原则":null,"4个阶段":null,"一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":"survey","方法论":"survey","发表情况":null,"链接":"https://arxiv.org/pdf/2408.11537","备注/讨论点":null,"父记录":null},{"标题":"A Survey of Robotic Navigation and Manipulation with Physics Simulators in the Era of Embodied AI","文章简称":null,"\\cite{}":null,"发表年月":null,"10大原则":null,"4个阶段":null,"一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":"survey","方法论":"survey","发表情况":null,"链接":"https://arxiv.org/pdf/2505.01458","备注/讨论点":null,"父记录":null},{"标题":"A Survey of Safe Reinforcement Learning and Constrained MDPs: A Technical Survey on Single-Agent and Multi-Agent Safety","文章简称":null,"\\cite{}":null,"发表年月":null,"10大原则":null,"4个阶段":null,"一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2505.17342","备注/讨论点":null,"父记录":null},{"标题":"A Survey on Bias and Fairness in Machine Learning","文章简称":null,"\\cite{}":null,"发表年月":null,"10大原则":null,"4个阶段":null,"一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/1908.09635","备注/讨论点":null,"父记录":null},{"标题":"Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action Models in Embodied Agents","文章简称":null,"\\cite{}":null,"发表年月":25.05,"10大原则":null,"4个阶段":null,"一作":"Zhejian Yang","一作单位":"Jiling University","通讯":null,"通讯单位":null,"具身任务":"VLA","方法论":"MLLM","发表情况":null,"链接":"https://arxiv.org/abs/2505.23450","备注/讨论点":null,"父记录":null},{"标题":"AgiBot World Colosseo: Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems","文章简称":null,"\\cite{}":null,"发表年月":25.03,"10大原则":null,"4个阶段":null,"一作":" Team AgiBot-World","一作单位":"HKU,  OpenDriveLab","通讯":" Team AgiBot-World","通讯单位":"HKU,  OpenDriveLab","具身任务":"Manipulation, VLA","方法论":"MLLM, Diffusion, 预训练, 指令微调","发表情况":null,"链接":"https://arxiv.org/abs/2503.06669","备注/讨论点":null,"父记录":null},{"标题":"Building Trustworthy AI: Transparent AI \nSystems via Language Models, Ontologies, and \nLogical Reasoning (TranspNet)","文章简称":null,"\\cite{}":null,"发表年月":24.12,"10大原则":null,"4个阶段":null,"一作":"Fadi Al Machot","一作单位":"Norwegian University of Life Science","通讯":null,"通讯单位":null,"具身任务":"LLM Reasoning","方法论":"LLM","发表情况":null,"链接":"https://arxiv.org/html/2411.08469v1","备注/讨论点":null,"父记录":null},{"标题":"CAN WE TRUST EMBODIED AGENTS? EXPLORING BACKDOOR ATTACKS AGAINST EMBODIED LLM-BASED DECISION-MAKING SYSTEMS","文章简称":null,"\\cite{}":null,"发表年月":24.05,"10大原则":null,"4个阶段":null,"一作":"Ruochen Jiao; Shaoyuan Xie","一作单位":"Northwestern University , University of California","通讯":"Ruochen Jiao; Shaoyuan Xie","通讯单位":"Northwestern University, University of California","具身任务":"VLA","方法论":"adversarial attack, VLA","发表情况":"ICLR 2025","链接":"https://arxiv.org/pdf/2405.20774","备注/讨论点":"决策规划,环境感知","父记录":null},{"标题":"CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning ","文章简称":null,"\\cite{}":null,"发表年月":25.06,"10大原则":null,"4个阶段":null,"一作":"Kailing Li","一作单位":"ECNU","通讯":null,"通讯单位":null,"具身任务":"EQA(2D)","方法论":"MLLM","发表情况":null,"链接":"https://arxiv.org/pdf/2506.17629","备注/讨论点":null,"父记录":null},{"标题":"Counterfactual collaborative reasoning","文章简称":"Ji et al.","\\cite{}":"ji2023counterfactual","发表年月":23.06,"10大原则":null,"4个阶段":null,"一作":"Jianchao Ji","一作单位":"Rutgers University","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/pdf/2307.00165","备注/讨论点":null,"父记录":null},{"标题":"Demystifying Embodied AI ","文章简称":null,"\\cite{}":null,"发表年月":25.01,"10大原则":null,"4个阶段":null,"一作":"Sunil Sable","一作单位":"Vishwakarma Institute of Technology","通讯":"S. Sable ","通讯单位":"Vishwakarma Institute of Technology","具身任务":"survey","方法论":"survey","发表情况":null,"链接":"https://link.springer.com/chapter/10.1007/978-3-031-68256-8_2","备注/讨论点":null,"父记录":null},{"标题":"DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following","文章简称":"DialFRED","\\cite{}":"gao2022dialfred","发表年月":22.07,"10大原则":"可信-准确","4个阶段":null,"一作":"Xiaofeng Gao","一作单位":"UCLA","通讯":null,"通讯单位":null,"具身任务":"EQA(2D)","方法论":"Benchmark, 强化学习","发表情况":"IEEE Robotics and Automation Letters, 2022","链接":"https://arxiv.org/abs/2202.13330","备注/讨论点":"关键词：指令对话","父记录":null},{"标题":" Diffusion Models for Robotic Manipulation: A Survey","文章简称":null,"\\cite{}":null,"发表年月":null,"10大原则":null,"4个阶段":null,"一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":"survey","方法论":"survey","发表情况":null,"链接":"https://arxiv.org/pdf/2504.08438","备注/讨论点":null,"父记录":null},{"标题":"Earbench: Towards evaluating physical risk awareness for task planning of foundation model-based embodied ai agents","文章简称":null,"\\cite{}":null,"发表年月":24.08,"10大原则":null,"4个阶段":null,"一作":"Zihao Zhu","一作单位":"CUHK-sz","通讯":"Baoyuan Wu","通讯单位":"CUHK-sz","具身任务":"Planning","方法论":"Benchmark, Prompt, LLM","发表情况":null,"链接":"https://arxiv.org/abs/2408.04449","备注/讨论点":"可能是最早的具身大脑的安全bench","父记录":null},{"标题":"Embodied AI with Two Arms: Zero-shot Learning, Safety and Modularity","文章简称":null,"\\cite{}":null,"发表年月":null,"10大原则":null,"4个阶段":null,"一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":"IROS","链接":"https://ieeexplore.ieee.org/abstract/document/10802181","备注/讨论点":null,"父记录":null},{"标题":"Embodied Agent Interface: Benchmarking LLMs for \nEmbodied Decision Making","文章简称":null,"\\cite{}":null,"发表年月":24.1,"10大原则":null,"4个阶段":null,"一作":"Manling Li,Shiyu Zhao,Qineng Wang,Kangrui Wang,Yu Zhou","一作单位":"Stanford University","通讯":null,"通讯单位":null,"具身任务":"Planning","方法论":"LLM, Benchmark","发表情况":null,"链接":"https://arxiv.org/abs/2410.07166","备注/讨论点":null,"父记录":null},{"标题":"Embodied artificial intelligence in ophthalmology","文章简称":null,"\\cite{}":null,"发表年月":25.06,"10大原则":null,"4个阶段":null,"一作":"Yao Qiu","一作单位":"The Hong Kong Polytechnic University","通讯":"Mingguang He","通讯单位":"The Hong Kong Polytechnic University","具身任务":"survey","方法论":"survey","发表情况":"Nature子刊npj | digital medicine","链接":"https://www.nature.com/articles/s41746-025-01754-4.pdf","备注/讨论点":null,"父记录":null},{"标题":"Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks","文章简称":null,"\\cite{}":null,"发表年月":25.03,"10大原则":null,"4个阶段":null,"一作":"Wenqi Zhang","一作单位":"ZJU","通讯":null,"通讯单位":null,"具身任务":"Planning, VLN","方法论":"LLM, 迭代式交互","发表情况":null,"链接":"https://arxiv.org/abs/2503.21696","备注/讨论点":null,"父记录":null},{"标题":"Embracing the Future: Navigating the Challenges and Solutions in Embodied Artificial Intelligence ","文章简称":null,"\\cite{}":null,"发表年月":25.01,"10大原则":null,"4个阶段":null,"一作":"Wasim Khan","一作单位":"Koneru Lakshmaiah Education Foundation","通讯":"W. Khan","通讯单位":"Koneru Lakshmaiah Education Foundation","具身任务":"survey","方法论":"survey, 理论框架","发表情况":null,"链接":"https://link.springer.com/chapter/10.1007/978-3-031-68256-8_13","备注/讨论点":null,"父记录":null},{"标题":"Empowering Autonomous Driving with Large Language Models: A Safety Perspective","文章简称":null,"\\cite{}":null,"发表年月":23.12,"10大原则":null,"4个阶段":null,"一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":"Auto Drive","方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2312.00812","备注/讨论点":null,"父记录":null},{"标题":"Ethical and regulatory challenges of AI technologies in healthcare: A narrative review","文章简称":null,"\\cite{}":null,"发表年月":24.02,"10大原则":null,"4个阶段":null,"一作":"Ciro Mennella","一作单位":"ICAR","通讯":"Umberto Maniscalco","通讯单位":"ICAR","具身任务":"survey","方法论":"survey","发表情况":null,"链接":"https://www.cell.com/heliyon/fulltext/S2405-8440(24)02328-4","备注/讨论点":null,"父记录":null},{"标题":"Everyday Object Meets Vision-and-Language Navigation Agent via Backdoor","文章简称":null,"\\cite{}":null,"发表年月":24.11,"10大原则":null,"4个阶段":null,"一作":"Keji He","一作单位":"SDU","通讯":"Liang Wang","通讯单位":"自动化所","具身任务":"VLN","方法论":"指令微调, MLLM","发表情况":"NeurIPS 2024","链接":"https://openreview.net/forum?id=rXGxbDJadh&referrer=%5Bthe%20profile%20of%20Keji%20He%5D(%2Fprofile%3Fid%3D~Keji_He1)","备注/讨论点":null,"父记录":null},{"标题":"Explainable Embodied Agents Through Social Cues: A Review","文章简称":null,"\\cite{}":null,"发表年月":21.07,"10大原则":null,"4个阶段":null,"一作":"SEBASTIAN WALLKÖTTER","一作单位":"Uppsala University and INESC-ID-Instituto Superior Técnico","通讯":null,"通讯单位":null,"具身任务":"survey","方法论":"survey","发表情况":"ACM Transactions on Human-Robot Interaction","链接":"https://dl.acm.org/doi/abs/10.1145/3457188","备注/讨论点":null,"父记录":null},{"标题":"Exploring Adversarial Obstacle Attacks in Search-based Path Planning for Autonomous Mobile Robots","文章简称":null,"\\cite{}":null,"发表年月":25.04,"10大原则":null,"4个阶段":null,"一作":"Adrian Szvoren;","一作单位":"UCL","通讯":"Dimitrios Kanoulas; Nilufer Tuptuk","通讯单位":"UCL","具身任务":"Navigation","方法论":"brute-force algorithm","发表情况":null,"链接":"https://arxiv.org/pdf/2504.06154","备注/讨论点":"行为规划","父记录":null},{"标题":"Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics","文章简称":null,"\\cite{}":null,"发表年月":24.11,"10大原则":null,"4个阶段":null,"一作":"Taowen Wang","一作单位":"Rochester Institute of Technology","通讯":"Ruixiang Tang","通讯单位":"Rutgers University","具身任务":"VLA","方法论":"VLA, adversarial attack","发表情况":null,"链接":"https://arxiv.org/pdf/2411.13587","备注/讨论点":"patch攻击","父记录":null},{"标题":"FP3: A 3D Foundation Policy for Robotic Manipulation","文章简称":null,"\\cite{}":null,"发表年月":25.03,"10大原则":null,"4个阶段":null,"一作":" Rujia Yang, Geng Chen","一作单位":"THU, PJLAB, UC San Diego","通讯":"Chuan Wen, Yang Gao","通讯单位":"THU, PJLAB, Shanghai Qi Zhi Institute","具身任务":"Manipulation","方法论":"Diffusion, Transformer, 指令微调, 预训练","发表情况":null,"链接":"https://arxiv.org/pdf/2503.08950","备注/讨论点":null,"父记录":null},{"标题":"Gripper Keypose and Object Pointflow as Interfaces for Bimanual Robotic Manipulation","文章简称":null,"\\cite{}":null,"发表年月":25.04,"10大原则":null,"4个阶段":null,"一作":"Yuyin Yang; Zetao Cai","一作单位":"FDU, ZJU, PJLAB","通讯":"Jiangmiao Pang","通讯单位":"PJLAB","具身任务":"Manipulation, VLA","方法论":"Diffusion, 预训练","发表情况":null,"链接":"https://arxiv.org/abs/2504.17784","备注/讨论点":null,"父记录":null},{"标题":"HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation","文章简称":null,"\\cite{}":null,"发表年月":25.05,"10大原则":null,"4个阶段":null,"一作":"Yi Li, Yuquan Deng, Jesse Zhang","一作单位":"NVIDIA, University of Washington, University of Southern California","通讯":"Anqi Li, Abhishek Gupta, Ankit Goyal","通讯单位":"NVIDIA, University of Washington","具身任务":"Manipulation","方法论":"MLLM, 模仿学习, 离线学习, 指令微调, 预训练, 规划算法","发表情况":null,"链接":"https://arxiv.org/pdf/2502.05485","备注/讨论点":null,"父记录":null},{"标题":"HASARD: A Benchmark for Vision-Based Safe Reinforcement Learning in Embodied Agents","文章简称":null,"\\cite{}":null,"发表年月":25.03,"10大原则":null,"4个阶段":null,"一作":"Tristan Tomilin","一作单位":"Eindhoven University of Technology","通讯":null,"通讯单位":null,"具身任务":"VLN, Planning","方法论":"Benchmark","发表情况":"ICLR2025","链接":"https://arxiv.org/abs/2503.08241","备注/讨论点":null,"父记录":null},{"标题":"HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments","文章简称":null,"\\cite{}":null,"发表年月":null,"10大原则":null,"4个阶段":null,"一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":"ICLR2024","链接":"https://arxiv.org/abs/2401.12975","备注/讨论点":null,"父记录":null},{"标题":"IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks","文章简称":"IS-Bench","\\cite{}":"lu2025bench","发表年月":25.06,"10大原则":"可信-准确","4个阶段":null,"一作":"Xiaoya Lu","一作单位":"PJLAB","通讯":"Jing Shao; Dongrui Liu","通讯单位":"PJLAB","具身任务":"Planning","方法论":"LLM, COT, Benchmark, OmniGibson模拟器","发表情况":null,"链接":"https://arxiv.org/abs/2506.16402","备注/讨论点":null,"父记录":null},{"标题":"IndustryEQA: Pushing the Frontiers of Embodied Question Answering in Industrial Scenarios","文章简称":null,"\\cite{}":null,"发表年月":25.05,"10大原则":null,"4个阶段":null,"一作":"Yifan Li","一作单位":"Michigan State University","通讯":"Kong Yu","通讯单位":"Michigan State University","具身任务":"EQA(2D)","方法论":"Benchmark, Isaac Sim模拟器","发表情况":null,"链接":"https://arxiv.org/abs/2505.20640","备注/讨论点":null,"父记录":null},{"标题":"KARMA: Augmenting Embodied AI Agents\nwith Long-and-short Term Memory Systems","文章简称":"KARMA","\\cite{}":"wangKARMAAugmentingEmbodied2025","发表年月":24.09,"10大原则":null,"4个阶段":null,"一作":"Zixuan Wang","一作单位":"CASIA","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/pdf/2409.14908","备注/讨论点":null,"父记录":null},{"标题":"Learning Safe Multi-Agent Control with Decentralized Neural Barrier Certificates","文章简称":null,"\\cite{}":null,"发表年月":null,"10大原则":null,"4个阶段":null,"一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2101.05436","备注/讨论点":null,"父记录":null},{"标题":"Learning Visually Grounded Domain Ontologies \nvia Embodied Conversation and Explanation","文章简称":"Park et al.","\\cite{}":"park2025learning","发表年月":25.04,"10大原则":null,"4个阶段":null,"一作":"Jonghyuk Park","一作单位":"University of Edinburgh","通讯":null,"通讯单位":null,"具身任务":"视觉理解","方法论":"Teacher Feedback","发表情况":"AAAI'25","链接":"https://ojs.aaai.org/index.php/AAAI/article/view/33573","备注/讨论点":null,"父记录":null},{"标题":"Learning to Explore using Active Neural SLAM","文章简称":null,"\\cite{}":"chaplotLearningExploreUsing2020","发表年月":20.04,"10大原则":null,"4个阶段":null,"一作":"Devendra Singh Chaplot","一作单位":"Carnegie Mellon University","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/pdf/2004.05155","备注/讨论点":null,"父记录":null},{"标题":"Legal and Ethical Consideration in Artificial Intelligence in Healthcare: Who Takes Responsibility?","文章简称":null,"\\cite{}":null,"发表年月":22.03,"10大原则":null,"4个阶段":null,"一作":"Nithesh Naik","一作单位":"Manipal Academy of Higher Education","通讯":null,"通讯单位":null,"具身任务":"survey","方法论":"survey","发表情况":null,"链接":"https://www.frontiersin.org/journals/surgery/articles/10.3389/fsurg.2022.862322/full?gclid=Cj0KCQi","备注/讨论点":null,"父记录":null},{"标题":"ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation ","文章简称":null,"\\cite{}":null,"发表年月":24.06,"10大原则":null,"4个阶段":null,"一作":"Xiaoqi Li","一作单位":"Peking University","通讯":null,"通讯单位":null,"具身任务":"VLA","方法论":"MLLM","发表情况":"CVPR'24","链接":"https://openaccess.thecvf.com/content/CVPR2024/papers/Li_ManipLLM_Embodied_Multimodal_Large_Language_Model_for_Object-Centric_Robotic_Manipulation_CVPR_2024_paper.pdf","备注/讨论点":null,"父记录":null},{"标题":"MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation","文章简称":null,"\\cite{}":null,"发表年月":25.03,"10大原则":null,"4个阶段":null,"一作":"Zhenyu Wu","一作单位":"Beijing University of Posts and Telecommunications","通讯":" Haibin Yan","通讯单位":"Beijing University of Posts and Telecommunications","具身任务":"Manipulation","方法论":"MLLM, 预训练, 指令微调, 规划算法","发表情况":null,"链接":"https://arxiv.org/pdf/2503.13446","备注/讨论点":null,"父记录":null},{"标题":"Multi-agent Embodied AI: Advances and Future Directions","文章简称":null,"\\cite{}":null,"发表年月":25.05,"10大原则":null,"4个阶段":null,"一作":"Zhaohan Feng","一作单位":"Beijing Institute of Technology","通讯":null,"通讯单位":null,"具身任务":"survey","方法论":"survey","发表情况":null,"链接":"https://arxiv.org/pdf/2505.05108?","备注/讨论点":null,"父记录":null},{"标题":"NoisyEQA: Benchmarking Embodied Question Answering Against Noisy Queries","文章简称":null,"\\cite{}":null,"发表年月":24.12,"10大原则":null,"4个阶段":null,"一作":"Tao Wu","一作单位":"NTU","通讯":null,"通讯单位":null,"具身任务":"EQA(2D)","方法论":"Benchmark","发表情况":null,"链接":"https://arxiv.org/abs/2412.10726","备注/讨论点":"关键词：感知噪声","父记录":null},{"标题":"Position: a call for embodied AI","文章简称":null,"\\cite{}":null,"发表年月":null,"10大原则":null,"4个阶段":null,"一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://openreview.net/forum?id=e5admkWKgV","备注/讨论点":null,"父记录":null},{"标题":"Provably Safe Deep Reinforcement Learning for Robotic Manipulation in Human Environments","文章简称":null,"\\cite{}":null,"发表年月":null,"10大原则":null,"4个阶段":null,"一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":"ICRA2022","链接":"https://arxiv.org/abs/2205.06311","备注/讨论点":null,"父记录":null},{"标题":"RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation","文章简称":null,"\\cite{}":null,"发表年月":25.03,"10大原则":null,"4个阶段":null,"一作":" Songming Liu; Lingxuan Wu","一作单位":"THU","通讯":"Hang Su; Jun Zhu","通讯单位":"THU","具身任务":"Manipulation, VLA","方法论":"Diffusion, 预训练, MLLM, LLM","发表情况":null,"链接":"https://arxiv.org/abs/2410.07864","备注/讨论点":null,"父记录":null},{"标题":"Random Spoofing Attack against LiDAR-Based Scan Matching SLAM","文章简称":null,"\\cite{}":null,"发表年月":24.02,"10大原则":null,"4个阶段":null,"一作":"Masashi Fukunaga","一作单位":"Mitsubishi Electric Corporation","通讯":"Takeshi Sugawara","通讯单位":"The University of Electro-Communications","具身任务":"SLAM","方法论":"对抗扰动","发表情况":"VehicleSec 2024","链接":"https://www.ndss-symposium.org/ndss-paper/auto-draft-476/","备注/讨论点":"环境感知","父记录":null},{"标题":"RiskAwareBench: Towards Evaluating Physical Risk Awareness for High-level Planning of LLM-based Embodied Agents","文章简称":null,"\\cite{}":null,"发表年月":24.08,"10大原则":null,"4个阶段":null,"一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/pdf/2408.04449v1","备注/讨论点":"EARBench的初版","父记录":null},{"标题":"RoboBERT: An End-to-end Multimodal Robotic Manipulation Model","文章简称":null,"\\cite{}":null,"发表年月":25.05,"10大原则":null,"4个阶段":null,"一作":" Sicheng Wang, Sheng Liu, Weiheng Wang","一作单位":"Casbot Robotic Corporation, Karlsruhe Institute of Technology","通讯":"Bin Fang","通讯单位":"Beijing Universuty of Post and Telecommunicate","具身任务":"Manipulation","方法论":"MLLM, Diffusion, ViT, 离线学习, 模仿学习, 指令微调, 预训练","发表情况":null,"链接":"https://arxiv.org/pdf/2502.07837","备注/讨论点":null,"父记录":null},{"标题":"RoboFlamingo-Plus: Fusion of Depth and RGB Perception with Vision-Language Models for Enhanced Robotic Manipulation","文章简称":null,"\\cite{}":null,"发表年月":25.03,"10大原则":null,"4个阶段":null,"一作":"Xiaojian Li","一作单位":null,"通讯":"Hangjie Mo","通讯单位":null,"具身任务":"Manipulation","方法论":"MLLM, VLM, 预训练, 指令微调","发表情况":null,"链接":"https://arxiv.org/pdf/2503.19510","备注/讨论点":null,"父记录":null},{"标题":"RoboGround: Robotic Manipulation with Grounded Vision-Language Priors","文章简称":null,"\\cite{}":null,"发表年月":25.04,"10大原则":null,"4个阶段":null,"一作":"Haifeng Huang; Xinyi Chen","一作单位":"ZJU, PJLAB","通讯":"Yilun Chen; ZhouZhao","通讯单位":"PJLAB, ZJU","具身任务":"Manipulation, VLA","方法论":"LLM, MLLM, 预训练, 指令微调","发表情况":null,"链接":"https://arxiv.org/abs/2504.21530","备注/讨论点":null,"父记录":null},{"标题":"Safe Bayesian Optimization for the Control of High-Dimensional Embodied Systems","文章简称":null,"\\cite{}":null,"发表年月":24.12,"10大原则":null,"4个阶段":null,"一作":"Yunyue Wei","一作单位":"THU","通讯":"Yanan Sui","通讯单位":"THU","具身任务":"VA","方法论":"贝叶斯优化","发表情况":"CoRL2024","链接":"https://arxiv.org/pdf/2412.20350","备注/讨论点":null,"父记录":null},{"标题":"Safe multi-agent reinforcement learning for multi-robot control","文章简称":null,"\\cite{}":null,"发表年月":null,"10大原则":null,"4个阶段":null,"一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://www.sciencedirect.com/science/article/abs/pii/S0004370223000516?via%3Dihub","备注/讨论点":null,"父记录":null},{"标题":"Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving","文章简称":null,"\\cite{}":null,"发表年月":null,"10大原则":null,"4个阶段":null,"一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/1610.03295","备注/讨论点":null,"父记录":null},{"标题":"SafeBench: A Benchmarking Platform for Safety Evaluation of Autonomous Vehicles","文章简称":null,"\\cite{}":null,"发表年月":22.06,"10大原则":null,"4个阶段":null,"一作":"Chejian Xu","一作单位":"UIUC","通讯":null,"通讯单位":null,"具身任务":"Auto Drive","方法论":"Benchmark, ROS","发表情况":"NIPS2022 Track on Datasets and Benchmarks.","链接":"https://arxiv.org/pdf/2206.09682","备注/讨论点":null,"父记录":null},{"标题":"SafeEmbodAI: a Safety Framework for Mobile Robots in Embodied AI Systems","文章简称":null,"\\cite{}":null,"发表年月":null,"10大原则":null,"4个阶段":null,"一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/pdf/2409.01630","备注/讨论点":null,"父记录":null},{"标题":"Securing Embodied AI: Addressing Cybersecurity Challenges in Physical Systems","文章简称":null,"\\cite{}":null,"发表年月":null,"10大原则":null,"4个阶段":null,"一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://link.springer.com/chapter/10.1007/978-3-031-68256-8_21","备注/讨论点":null,"父记录":null},{"标题":"Securing the Future: Exploring Privacy Risks and Security Questions in Robotic Systems","文章简称":null,"\\cite{}":null,"发表年月":null,"10大原则":null,"4个阶段":null,"一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/2409.09972","备注/讨论点":null,"父记录":null},{"标题":"Security Considerations in AI-Robotics:\nA Survey of Current Methods,\nChallenges, and Opportunities","文章简称":null,"\\cite{}":null,"发表年月":24.01,"10大原则":null,"4个阶段":null,"一作":"SUBASH NEUPANE","一作单位":"Mississippi State University","通讯":"Subash Neupane","通讯单位":"Mississippi State University","具身任务":"survey, Manipulation","方法论":"survey, MLLM","发表情况":null,"链接":"https://arxiv.org/pdf/2310.08565","备注/讨论点":null,"父记录":null},{"标题":"Shake-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Manipulations and Liquid Mixing","文章简称":null,"\\cite{}":null,"发表年月":25.01,"10大原则":null,"4个阶段":null,"一作":"Muhamamd Haris Khan; Selamawit Asfaw","一作单位":"Skoltech","通讯":null,"通讯单位":null,"具身任务":"Manipulation, VLA","方法论":"MLLM, LLM","发表情况":null,"链接":"https://arxiv.org/abs/2501.06919","备注/讨论点":null,"父记录":null},{"标题":"Situated Instruction Following","文章简称":"SIF","\\cite{}":"yenamandraHomeRobotOpenVocabularyMobile2024","发表年月":24.07,"10大原则":null,"4个阶段":null,"一作":"So Yeon Min","一作单位":"CMU","通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://arxiv.org/pdf/2407.12061","备注/讨论点":null,"父记录":null},{"标题":"Strategic Planning of Stealthy Backdoor Attacks in Markov Decision Processes","文章简称":null,"\\cite{}":null,"发表年月":25.04,"10大原则":null,"4个阶段":null,"一作":"Xinyi Wei","一作单位":"UFL","通讯":"Jie Fu","通讯单位":"UFL","具身任务":"Manipulation","方法论":"强化学习","发表情况":null,"链接":"https://arxiv.org/pdf/2504.13276","备注/讨论点":"行为规划","父记录":null},{"标题":"Subtle Risks, Critical Failures: A Framework for Diagnosing Physical Safety of LLMs for Embodied Decision Making","文章简称":null,"\\cite{}":null,"发表年月":25.05,"10大原则":null,"4个阶段":null,"一作":"Yejin Son","一作单位":"Yonsei University","通讯":"Youngjae Yu","通讯单位":"Yonsei University","具身任务":"Planning","方法论":"LLM, Benchmark","发表情况":null,"链接":"https://arxiv.org/abs/2505.19933","备注/讨论点":null,"父记录":null},{"标题":"Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours","文章简称":null,"\\cite{}":null,"发表年月":15.09,"10大原则":null,"4个阶段":null,"一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":"Manipulation","方法论":null,"发表情况":null,"链接":"https://arxiv.org/abs/1509.06825","备注/讨论点":null,"父记录":null},{"标题":"TLA: Tactile-Language-Action Model for Contact-Rich Manipulation","文章简称":null,"\\cite{}":null,"发表年月":25.03,"10大原则":null,"4个阶段":null,"一作":" Peng Hao, Chaofan Zhang","一作单位":"Samsung Research China, Institute of Automation,  Chinese Academy of Sciences","通讯":null,"通讯单位":null,"具身任务":"Manipulation","方法论":"MLLM, 模仿学习, 离线学习, 指令微调, 预训练","发表情况":null,"链接":"https://arxiv.org/pdf/2503.08548","备注/讨论点":null,"父记录":null},{"标题":"Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation","文章简称":null,"\\cite{}":null,"发表年月":25.06,"10大原则":null,"4个阶段":null,"一作":" Yuanqi Yao, Yan Ding, Siao Liu, Bin Zhao, Haoming Song","一作单位":"PJLAB, FDU, SJTU, INSAIT,  Sofia University","通讯":" Dong Wang","通讯单位":"PJLAB","具身任务":"Manipulation","方法论":"Diffusion, 离线学习","发表情况":null,"链接":"https://arxiv.org/pdf/2504.00420","备注/讨论点":null,"父记录":null},{"标题":"Towards Explainable Embodied AI","文章简称":null,"\\cite{}":null,"发表年月":21.08,"10大原则":null,"4个阶段":null,"一作":"Vidhi Jain ","一作单位":"CMU","通讯":null,"通讯单位":null,"具身任务":"Planning","方法论":"Feature Atrribution","发表情况":"硕士论文","链接":"https://www.ri.cmu.edu/app/uploads/2021/08/MSRoboticsThesis-2.pdf","备注/讨论点":null,"父记录":null},{"标题":"Towards Explainable and Trustworthy Collaborative Robots through Embodied Question Answering","文章简称":null,"\\cite{}":null,"发表年月":22.05,"10大原则":null,"4个阶段":null,"一作":"Lars Kunze","一作单位":"University of Oxford","通讯":null,"通讯单位":null,"具身任务":"EQA(2D)","方法论":"Knowledge Graph","发表情况":null,"链接":"https://ora.ox.ac.uk/objects/uuid:4d8079cb-7633-417d-ba9b-d580aaebff64","备注/讨论点":null,"父记录":null},{"标题":"Unintended Consequences of Biased Robotic and Artificial Intelligence Systems [Ethical, Legal, and Societal Issues]","文章简称":null,"\\cite{}":null,"发表年月":19.09,"10大原则":null,"4个阶段":null,"一作":"Ludovic Righetti","一作单位":"NYU","通讯":null,"通讯单位":null,"具身任务":"survey","方法论":"理论框架","发表情况":null,"链接":"https://ieeexplore.ieee.org/document/8825881","备注/讨论点":null,"父记录":null},{"标题":"VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive Token Caching in Robotic Manipulation","文章简称":null,"\\cite{}":null,"发表年月":25.02,"10大原则":null,"4个阶段":null,"一作":"Siyu Xu","一作单位":"USYD","通讯":" Chang Xu","通讯单位":"USYD","具身任务":"Manipulation","方法论":"MLLM, 模仿学习, 离线学习, 指令微调","发表情况":null,"链接":"https://arxiv.org/pdf/2502.02175","备注/讨论点":null,"父记录":null},{"标题":"VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning","文章简称":null,"\\cite{}":null,"发表年月":25.05,"10大原则":null,"4个阶段":null,"一作":"Guanxing Lu","一作单位":"Tsinghua Shenzhen International Graduate School,  Tsinghua University","通讯":"Yansong Tang","通讯单位":"Tsinghua Shenzhen International Graduate School,  Tsinghua University","具身任务":"Manipulation","方法论":"MLLM, 强化学习, 模仿学习, 在线学习, 离线学习, 预训练, 指令微调","发表情况":null,"链接":"https://arxiv.org/pdf/2505.18719","备注/讨论点":null,"父记录":null},{"标题":"VOYAGER: An Open-Ended Embodied Agent with Large Language Models ","文章简称":null,"\\cite{}":null,"发表年月":23.05,"10大原则":null,"4个阶段":null,"一作":"Guanzhi Wang","一作单位":"NVIDIA","通讯":null,"通讯单位":null,"具身任务":"Embodied Task","方法论":"LLM","发表情况":null,"链接":"https://arxiv.org/pdf/2305.16291","备注/讨论点":null,"父记录":null},{"标题":"World Models: The Safety Perspective","文章简称":null,"\\cite{}":null,"发表年月":null,"10大原则":null,"4个阶段":null,"一作":null,"一作单位":null,"通讯":null,"通讯单位":null,"具身任务":null,"方法论":null,"发表情况":null,"链接":"https://ieeexplore.ieee.org/abstract/document/10771431","备注/讨论点":"世界模型安全（用于自动驾驶）","父记录":null},{"标题":"iManip: Skill-Incremental Learning for Robotic Manipulation","文章简称":null,"\\cite{}":null,"发表年月":25.03,"10大原则":null,"4个阶段":null,"一作":" Zexin Zheng","一作单位":"Sun Yat-sen University","通讯":null,"通讯单位":null,"具身任务":"Manipulation","方法论":"ViT, Transformer, 模仿学习, 离线学习, 增量学习","发表情况":null,"链接":"https://arxiv.org/pdf/2503.07087","备注/讨论点":null,"父记录":null}]},3632:function(n,e,l){"use strict";var t=l(3751),a=l(641);function i(n,e,l,t,i,o){const r=(0,a.g2)("ProjectPage"),s=(0,a.g2)("v-app");return(0,a.uX)(),(0,a.Wv)(s,null,{default:(0,a.k6)(()=>[(0,a.bF)(r)]),_:1})}var o=l(33);const r={class:"search-container"},s={key:0,class:"search-stats"},u={class:"tag-matrix-container"},c={class:"search-bar"},d={key:0,class:"active-filters"},g={class:"text-center"},p={class:"content-frame"},f={class:"mt-4"};function h(n,e,l,i,h,m){const v=(0,a.g2)("paper-intro"),b=(0,a.g2)("v-icon"),y=(0,a.g2)("v-chip"),A=(0,a.g2)("v-text-field"),S=(0,a.g2)("v-btn"),L=(0,a.g2)("tag-matrix"),k=(0,a.g2)("v-divider"),w=(0,a.g2)("PaperCard"),M=(0,a.g2)("v-col"),E=(0,a.g2)("v-row"),x=(0,a.g2)("v-container"),C=(0,a.g2)("FloatingFeedback"),R=(0,a.g2)("ImpactMetrics"),T=(0,a.g2)("BibTeXSection");return(0,a.uX)(),(0,a.CE)(a.FK,null,[(0,a.bF)(x,{class:"project-page",fluid:""},{default:(0,a.k6)(()=>[(0,a.bF)(v,{paper:h.intro,class:"mb-6"},null,8,["paper"]),e[12]||(e[12]=(0,a.Lk)("div",{class:"header"},[(0,a.Lk)("h1",{class:"title"},[(0,a.Lk)("span",{class:"title-decor","data-text":"Awesome"},"Awesome"),(0,a.eW)(),(0,a.Lk)("span",{class:"title-chip"},"Trustworthy"),(0,a.eW)(),(0,a.Lk)("span",{class:"title-decor","data-text":"Embodied-AI"},"Embodied-AI")])],-1)),(0,a.Lk)("div",r,[h.searchKeyword||h.selectedTags.length>0?((0,a.uX)(),(0,a.CE)("div",s,[(0,a.bF)(y,{small:"",color:"blue lighten-4","text-color":"blue darken-3",class:"ma-1"},{default:(0,a.k6)(()=>[(0,a.bF)(b,{small:"",left:""},{default:(0,a.k6)(()=>e[2]||(e[2]=[(0,a.eW)("mdi-file-document-multiple")])),_:1,__:[2]}),(0,a.eW)(" "+(0,o.v_)(h.filteredPapers.length)+" papers found ",1)]),_:1}),h.searchKeyword?((0,a.uX)(),(0,a.Wv)(y,{key:0,small:"",color:"green lighten-4","text-color":"green darken-3",class:"ma-1",close:"","onClick:close":m.clearSearch},{default:(0,a.k6)(()=>[(0,a.bF)(b,{small:"",left:""},{default:(0,a.k6)(()=>e[3]||(e[3]=[(0,a.eW)("mdi-magnify")])),_:1,__:[3]}),(0,a.eW)(' "'+(0,o.v_)(h.searchKeyword)+'" ',1)]),_:1},8,["onClick:close"])):(0,a.Q3)("",!0)])):(0,a.Q3)("",!0),(0,a.Lk)("div",u,[((0,a.uX)(),(0,a.Wv)(L,{onTagClick:m.handleTagFilter,"rows-data":h.papers,onFiltered:m.onMatrixFiltered,key:h.tmxKey},{"left-actions":(0,a.k6)(()=>[(0,a.Lk)("div",c,[(0,a.bF)(A,{modelValue:h.searchKeyword,"onUpdate:modelValue":e[0]||(e[0]=n=>h.searchKeyword=n),class:"search-input",dense:"",outlined:"","prepend-inner-icon":"mdi-magnify",placeholder:"Search papers by title, author...",clearable:"",onKeyup:(0,t.jR)(m.filterPapers,["enter"]),"hide-details":""},null,8,["modelValue","onKeyup"]),(0,a.bF)(S,{class:"search-btn",small:"",elevation:"0",onClick:m.filterPapers},{default:(0,a.k6)(()=>e[4]||(e[4]=[(0,a.eW)(" Search ")])),_:1,__:[4]},8,["onClick"])])]),"footer-right":(0,a.k6)(()=>[(0,a.Lk)("button",{class:"tmx-toggle",onClick:e[1]||(e[1]=n=>h.showPaperList=!h.showPaperList)},[h.showPaperList?((0,a.uX)(),(0,a.CE)(a.FK,{key:0},[(0,a.eW)("▲ HIDE PAPERS")],64)):((0,a.uX)(),(0,a.CE)(a.FK,{key:1},[(0,a.eW)("▼ SHOW PAPERS ("+(0,o.v_)(h.filteredPapers.length)+")",1)],64))])]),_:1},8,["onTagClick","rows-data","onFiltered"])),h.activeFilter?((0,a.uX)(),(0,a.CE)("div",d,[(0,a.bF)(k,{class:"my-4"}),(0,a.Lk)("div",g,[(0,a.bF)(y,{color:"blue",dark:"",close:"","onClick:close":m.clearTagFilter},{default:(0,a.k6)(()=>[(0,a.bF)(b,{small:"",left:""},{default:(0,a.k6)(()=>e[5]||(e[5]=[(0,a.eW)("mdi-filter")])),_:1,__:[5]}),(0,a.eW)(" "+(0,o.v_)(h.activeFilter),1)]),_:1},8,["onClick:close"])])])):(0,a.Q3)("",!0)])]),(0,a.Lk)("div",p,[(0,a.bo)((0,a.bF)(E,{justify:"center"},{default:(0,a.k6)(()=>[((0,a.uX)(!0),(0,a.CE)(a.FK,null,(0,a.pI)(h.filteredPapers,(n,e)=>((0,a.uX)(),(0,a.Wv)(M,{cols:"12",lg:"11",xl:"11",key:n.title+(n.date||e)},{default:(0,a.k6)(()=>[(0,a.bF)(w,{title:n.title,principleTag:n.principleTag,stageTag:n.stageTag,firstAuthor:n.author,link:n.link||"#",pubDate:String(n.date??n["发表年月"]??n["发表时间"]??""),"font-scale":.9},null,8,["title","principleTag","stageTag","firstAuthor","link","pubDate"])]),_:2},1024))),128))]),_:1},512),[[t.aG,h.showPaperList]])]),0===h.filteredPapers.length?((0,a.uX)(),(0,a.Wv)(E,{key:0,justify:"center"},{default:(0,a.k6)(()=>[(0,a.bF)(M,{cols:"12",class:"empty-state"},{default:(0,a.k6)(()=>[(0,a.bF)(b,{"x-large":"",color:"grey lighten-1"},{default:(0,a.k6)(()=>e[6]||(e[6]=[(0,a.eW)("mdi-file-search-outline")])),_:1,__:[6]}),e[11]||(e[11]=(0,a.Lk)("p",{class:"mt-4"},"No papers found matching your criteria",-1)),(0,a.Lk)("div",f,[(0,a.bF)(S,{color:"blue lighten-2",text:"",class:"mr-3",onClick:m.resetFilters},{default:(0,a.k6)(()=>[(0,a.bF)(b,{left:""},{default:(0,a.k6)(()=>e[7]||(e[7]=[(0,a.eW)("mdi-refresh")])),_:1,__:[7]}),e[8]||(e[8]=(0,a.eW)(" Reset Filters "))]),_:1,__:[8]},8,["onClick"]),(0,a.bF)(S,{color:"green lighten-2",text:"",onClick:m.showAllPapers},{default:(0,a.k6)(()=>[(0,a.bF)(b,{left:""},{default:(0,a.k6)(()=>e[9]||(e[9]=[(0,a.eW)("mdi-eye")])),_:1,__:[9]}),e[10]||(e[10]=(0,a.eW)(" Show All Papers "))]),_:1,__:[10]},8,["onClick"])])]),_:1,__:[11]})]),_:1})):(0,a.Q3)("",!0)]),_:1,__:[12]}),(0,a.bF)(C,{fixed:"","offset-x":16,"offset-y":16,"reserve-below":0,"button-height":36}),(0,a.bF)(R,{"stars-repo":"owner/repo","updated-repo":"owner/repo","visitors-api":"https://old-union-b7eb.3034297530.workers.dev/visitors","collect-api":"https://old-union-b7eb.3034297530.workers.dev/collect","top-n":5}),(0,a.bF)(x,{class:"py-8"},{default:(0,a.k6)(()=>[(0,a.bF)(E,{justify:"center"},{default:(0,a.k6)(()=>[(0,a.bF)(M,{cols:"12",sm:"11",md:"10",lg:"9",xl:"9"},{default:(0,a.k6)(()=>[(0,a.bF)(T,{class:"mt-8"})]),_:1})]),_:1})]),_:1})],64)}l(8111),l(2489),l(7588),l(1701),l(3579),l(7642),l(8004),l(3853),l(5876),l(2475),l(5024),l(1698);var m="data:image/png;base64,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",v=l.p+"img/ecnu.4992016f.png",b=l.p+"img/thu.54d19a61.png";const y={class:"intro"},A={class:"title"},S={class:"authors"},L=["href"],k={key:0,class:"mark"},w={key:0,class:"sep"},M={key:1},E={class:"aff-row"},x={class:"aff-names",title:""},C={class:"aff-names",title:""},R={class:"aff-names",title:""},T={class:"hero-image"},I={class:"figure2-section f2-compact",id:"figure2"},P={class:"f2-figure"},D=["src"];function F(n,e,l,t,i,r){const s=(0,a.g2)("v-img");return(0,a.uX)(),(0,a.CE)("section",y,[(0,a.Lk)("h1",A,(0,o.v_)(l.paper.title),1),(0,a.Lk)("div",S,[((0,a.uX)(!0),(0,a.CE)(a.FK,null,(0,a.pI)(l.paper.authors||[],(n,e)=>((0,a.uX)(),(0,a.CE)(a.FK,{key:n.name},[(0,a.Lk)("a",{href:n.homepage,target:"_blank",rel:"noopener",class:"author"},[(0,a.eW)((0,o.v_)(n.name),1),r.getMark(n)?((0,a.uX)(),(0,a.CE)("sup",k,(0,o.v_)(r.getMark(n)),1)):(0,a.Q3)("",!0)],8,L),e<l.paper.authors.length-1?((0,a.uX)(),(0,a.CE)("span",w," ,")):(0,a.Q3)("",!0),"Zhihao Luo"===n.name?((0,a.uX)(),(0,a.CE)("br",M)):(0,a.Q3)("",!0)],64))),128))]),(0,a.Lk)("div",E,[e[0]||(e[0]=(0,a.Lk)("a",{href:"https://www.shlab.org.cn/",target:"_blank",rel:"noopener",class:"aff-logo-link","aria-label":"Shanghai AI Lab"},[(0,a.Lk)("img",{src:m,alt:"Shanghai Artificial Intelligence Laboratory",class:"aff-logo"})],-1)),(0,a.Lk)("div",x,[(0,a.Lk)("div",{ref:"affCn",class:"aff-cn",style:(0,o.Tr)({width:i.affTargetWidth+"px",letterSpacing:i.affCnLetterSpacing+"px"})}," 上海人工智能实验室 ",4),(0,a.Lk)("div",{ref:"affEn",class:"aff-en",style:(0,o.Tr)({width:i.affTargetWidth+"px",wordSpacing:i.affEnWordSpacing+"px",letterSpacing:i.affEnLetterSpacing+"px"})}," Shanghai Artificial Intelligence Laboratory ",4)]),e[1]||(e[1]=(0,a.Lk)("div",{style:{width:"160px"}},null,-1)),e[2]||(e[2]=(0,a.Lk)("a",{href:"https://www.ecnu.edu.cn/",target:"_blank",rel:"noopener",class:"aff-logo-link","aria-label":"East China Normal University"},[(0,a.Lk)("img",{src:v,alt:"East China Normal University",class:"aff-logo"})],-1)),(0,a.Lk)("div",C,[(0,a.Lk)("div",{ref:"affCn",class:"aff-cn",style:(0,o.Tr)({width:i.affTargetWidth+"px",letterSpacing:i.affCnLetterSpacing+"px"})}," 华东师范大学 ",4),(0,a.Lk)("div",{ref:"affEn",class:"aff-en",style:(0,o.Tr)({width:i.affTargetWidth+"px",wordSpacing:i.affEnWordSpacing+"px",letterSpacing:i.affEnLetterSpacing+"px"})}," East China Normal University ",4)]),e[3]||(e[3]=(0,a.Lk)("div",{style:{width:"70px"}},null,-1)),e[4]||(e[4]=(0,a.Lk)("a",{href:"https://www.tsinghua.edu.cn/",target:"_blank",rel:"noopener",class:"aff-logo-link","aria-label":"Tsinghua University"},[(0,a.Lk)("img",{src:b,alt:"Tsinghua University",class:"aff-logo"})],-1)),(0,a.Lk)("div",R,[(0,a.Lk)("div",{ref:"affCn",class:"aff-cn",style:(0,o.Tr)({width:i.affTargetWidth+"px",letterSpacing:i.affCnLetterSpacing+"px"})}," 清华大学 ",4),(0,a.Lk)("div",{ref:"affEn",class:"aff-en",style:(0,o.Tr)({width:i.affTargetWidth+"px",wordSpacing:i.affEnWordSpacing+"px",letterSpacing:i.affEnLetterSpacing+"px"})}," Tsinghua University ",4)])]),e[10]||(e[10]=(0,a.Fv)('<section class="abs-band" data-v-7dfce6ad><div class="abs-decorations" data-v-7dfce6ad><div class="abs-line abs-line-1" data-v-7dfce6ad></div><div class="abs-line abs-line-2" data-v-7dfce6ad></div><div class="abs-dot abs-dot-1" data-v-7dfce6ad></div><div class="abs-dot abs-dot-2" data-v-7dfce6ad></div><div class="abs-dot abs-dot-3" data-v-7dfce6ad></div><div class="abs-gradient-orb abs-orb-1" data-v-7dfce6ad></div><div class="abs-gradient-orb abs-orb-2" data-v-7dfce6ad></div></div><div class="abs-inner" data-v-7dfce6ad><div class="abs-title-wrapper" data-v-7dfce6ad><div class="abs-title-line" data-v-7dfce6ad></div><h2 class="abs-title" data-v-7dfce6ad>Abstract</h2><div class="abs-title-line" data-v-7dfce6ad></div></div><p class="abs-text" lang="en" data-v-7dfce6ad>The increasing autonomy and physical capability of Embodied Artificial Intelligence (EAI) introduce critical challenges to safety and trustworthiness. Unlike purely digital AI, failures in perception, planning, or interaction can lead to direct physical harm, property damage, or the violation of human safety and social norms. However, current EAI foundation models disregard the risks of misalignment between the model capabilities and the safety and trustworthiness competencies. Some works attempt to address these issues, however, they lack a unified framework capable of balancing the developmental trajectories between safety and capability. In this paper, we first comprehensively define a new term <em data-v-7dfce6ad>safe and trustworthy EAI</em> by establishing an L1-L5 levels framework and proposing ten core principles of trustworthiness and safety. To unify fragmented research efforts, we propose a novel, agent-centric framework that analyzes risks across the four operational stages of an EAI system. We systematically review state-of-the-art but fragmented solutions, benchmarks, and evaluation metrics, identifying key gaps and challenges. Finally, we identify the need for a paradigm shift away from optimizing isolated components towards a holistic, cybernetic approach. We argue that future progress hinges on engineering the closed-loop system of the agent (Self), its environment (World), and their dynamic coupling (Interaction), paving the way for the next generation of truly safe and trustworthy EAI.</p></div></section>',1)),(0,a.Lk)("figure",T,[(0,a.bF)(s,{src:r.withBase("images/figure1.png"),contain:"",class:"full-img"},null,8,["src"]),e[5]||(e[5]=(0,a.Lk)("figcaption",{class:"caption"}," Figure 1. Capability-Safety divergence in the Embodied AI landscape ",-1))]),e[11]||(e[11]=(0,a.Fv)('<div class="motivation-block" data-v-7dfce6ad><h2 class="section-title" data-v-7dfce6ad>Research Motivation</h2><p class="section-text" style="margin-bottom:8px;" data-v-7dfce6ad> While embodied AI (EAI) products are rapidly improving in capability, they often lack reliable safety mechanisms. In contrast, academic safety research remains fragmented and lags behind in capability. This divergence has raised significant concerns about the trustworthiness of physical AI systems.</p><p class="section-text" style="margin-top:0;" data-v-7dfce6ad> To address this gap, we propose a unified research framework that integrates capability advancement and trustworthy safety, paving the way for the development of safe and aligned embodied agents. </p><div class="tags" data-v-7dfce6ad><span class="tag" data-v-7dfce6ad>EAI Safety</span><span class="tag" data-v-7dfce6ad>Capability-Safety Gap</span><span class="tag" data-v-7dfce6ad>Trustworthy AI</span><span class="tag" data-v-7dfce6ad>Embodied Intelligence</span></div></div>',1)),(0,a.Lk)("section",I,[e[7]||(e[7]=(0,a.Lk)("h2",{class:"f2-title"},"The Five Levels of “Make Safe EAI”",-1)),e[8]||(e[8]=(0,a.Lk)("p",{class:"f2-lead"},[(0,a.eW)(" We chart the progression from "),(0,a.Lk)("strong",null,"Resistance"),(0,a.eW)(" (L1–L2) to "),(0,a.Lk)("strong",null,"Resilience"),(0,a.eW)(" (L3–L5), moving from refusal and oversight to adaptive learning and verifiable guarantees. ")],-1)),(0,a.Lk)("figure",P,[(0,a.Lk)("img",{src:r.withBase("images/figure2.png"),alt:"Figure 2: The five levels of Make Safe EAI"},null,8,D),e[6]||(e[6]=(0,a.Lk)("figcaption",null,"Figure 2. From Resistance (L1–L2) to Resilience (L3–L5).",-1))]),e[9]||(e[9]=(0,a.Fv)('<div class="f2-row f2-row-top" data-v-7dfce6ad><article class="f2-card" data-v-7dfce6ad><h3 data-v-7dfce6ad>L1 — Alignment <span data-v-7dfce6ad>(Foundational Resistance)</span></h3><ul class="f2-bullets" data-v-7dfce6ad><li data-v-7dfce6ad><span class="dot" data-v-7dfce6ad>•</span><span class="label" data-v-7dfce6ad>Goal</span><span class="text" data-v-7dfce6ad>Refuse harmful instructions; follow basic safety norms.</span></li><li data-v-7dfce6ad><span class="dot" data-v-7dfce6ad>•</span><span class="label" data-v-7dfce6ad>How</span><span class="text" data-v-7dfce6ad>Instruction tuning, RLHF, safety filters, red-teaming data.</span></li><li data-v-7dfce6ad><span class="dot" data-v-7dfce6ad>•</span><span class="label" data-v-7dfce6ad>Limit</span><span class="text" data-v-7dfce6ad>Correlation-based; vulnerable to jailbreaks and shifts.</span></li></ul></article><article class="f2-card" data-v-7dfce6ad><h3 data-v-7dfce6ad>L2 — Intervention <span data-v-7dfce6ad>(Oversight-based Resistance)</span></h3><ul class="f2-bullets" data-v-7dfce6ad><li data-v-7dfce6ad><span class="dot" data-v-7dfce6ad>•</span><span class="label" data-v-7dfce6ad>Goal</span><span class="text" data-v-7dfce6ad>Let humans halt/redirect <i data-v-7dfce6ad>before</i> risky actions.</span></li><li data-v-7dfce6ad><span class="dot" data-v-7dfce6ad>•</span><span class="label" data-v-7dfce6ad>How</span><span class="text" data-v-7dfce6ad>Interrupt channels, intent/plan display, trajectory visualization.</span></li><li data-v-7dfce6ad><span class="dot" data-v-7dfce6ad>•</span><span class="label" data-v-7dfce6ad>Limit</span><span class="text" data-v-7dfce6ad>Needs constant oversight; weak scalability to high autonomy.</span></li></ul></article></div><div class="f2-row f2-row-bottom" data-v-7dfce6ad><article class="f2-card" data-v-7dfce6ad><h3 data-v-7dfce6ad>L3 — Mimetic Reflection <span data-v-7dfce6ad>(Foundational Resilience)</span></h3><ul class="f2-bullets" data-v-7dfce6ad><li data-v-7dfce6ad><span class="dot" data-v-7dfce6ad>•</span><span class="label" data-v-7dfce6ad>Goal</span><span class="text" data-v-7dfce6ad>Internalize validated safe behaviors.</span></li><li data-v-7dfce6ad><span class="dot" data-v-7dfce6ad>•</span><span class="label" data-v-7dfce6ad>How</span><span class="text" data-v-7dfce6ad>Imitation learning, behavior cloning, curated safety playbooks.</span></li><li data-v-7dfce6ad><span class="dot" data-v-7dfce6ad>•</span><span class="label" data-v-7dfce6ad>Limit</span><span class="text" data-v-7dfce6ad>Limited generalization to novel tasks and combinations.</span></li></ul></article><article class="f2-card" data-v-7dfce6ad><h3 data-v-7dfce6ad>L4 — Evolutionary Reflection <span data-v-7dfce6ad>(Adaptive Resilience)</span></h3><ul class="f2-bullets" data-v-7dfce6ad><li data-v-7dfce6ad><span class="dot" data-v-7dfce6ad>•</span><span class="label" data-v-7dfce6ad>Goal</span><span class="text" data-v-7dfce6ad>Continual self-improvement; proactive patching.</span></li><li data-v-7dfce6ad><span class="dot" data-v-7dfce6ad>•</span><span class="label" data-v-7dfce6ad>How</span><span class="text" data-v-7dfce6ad>Continual learning, self red-teaming, safety-aware exploration.</span></li><li data-v-7dfce6ad><span class="dot" data-v-7dfce6ad>•</span><span class="label" data-v-7dfce6ad>Limit</span><span class="text" data-v-7dfce6ad>Empirical assurance only — no prior formal guarantees.</span></li></ul></article><article class="f2-card" data-v-7dfce6ad><h3 data-v-7dfce6ad>L5 — Verifiable Reflection <span data-v-7dfce6ad>(Guaranteed Resilience)</span></h3><ul class="f2-bullets" data-v-7dfce6ad><li data-v-7dfce6ad><span class="dot" data-v-7dfce6ad>•</span><span class="label" data-v-7dfce6ad>Goal</span><span class="text" data-v-7dfce6ad>Provable safety/stability of closed-loop behavior.</span></li><li data-v-7dfce6ad><span class="dot" data-v-7dfce6ad>•</span><span class="label" data-v-7dfce6ad>How</span><span class="text" data-v-7dfce6ad>Reachability/invariance analysis, control-theoretic synthesis, neuro-symbolic proofs.</span></li><li data-v-7dfce6ad><span class="dot" data-v-7dfce6ad>•</span><span class="label" data-v-7dfce6ad>Limit</span><span class="text" data-v-7dfce6ad>Model/compute-intensive; engineering maturity evolving.</span></li></ul></article></div>',2))])])}const N="/Awesome-Trustworthy-Embodied-AI/",U=n=>N+(n.startsWith("/")?n.slice(1):n);var _={name:"PaperIntro",props:{paper:{type:Object,default:()=>({title:"",authors:[],affiliations:[]})}},data(){return{affTargetWidth:0,affCnLetterSpacing:0,affEnWordSpacing:0,affEnLetterSpacing:0}},mounted(){this.$nextTick(()=>{this.affMeasure(),window.addEventListener("resize",this.affMeasure)})},beforeUnmount(){window.removeEventListener("resize",this.affMeasure)},computed:{resolvedImages(){const n=this.paper.images||[];return n.map(n=>this.resolveImg(n))}},methods:{withBase:U,getMark(n){if(!n)return"";if(n.symbol&&"string"===typeof n.symbol&&n.symbol.trim())return n.symbol.trim();const e=(n.equalSymbol||n.equal||"").toString().toLowerCase().trim();return"dagger"===e||"†"===e||"dag"===e?"†":"section"===e||"§"===e||"sect"===e?"§":"asterisk"===e||"star"===e||"*"===e||"true"===e||!0===n.equal?"*":""},resolveImg(n){if(!n)return"";if(/^https?:\/\//i.test(n))return n;const e=n.replace(/^\/?assets\//,"");try{return new URL(`../assets/${e}`,"file:///E:/show/src/components/PaperIntro.vue%3Fvue&type=script&lang=js").href}catch{return n}},affCountGaps(n){const e=(n||"").replace(/\s+/g,"").replace(/[，。、“”‘’·—\-·・]/g,"");return Math.max(e.length-1,1)},affCountEnSpaces(n){return((n||"").match(/ /g)||[]).length},affCountEnGaps(n){const e=(n||"").trim();return Math.max(e.length-1,1)},affMeasure(){const n=this.$refs.affCn,e=this.$refs.affEn;if(!n||!e)return;const l=n.style.letterSpacing,t=n.style.width,a=e.style.width,i=e.style.wordSpacing,o=e.style.letterSpacing;n.style.letterSpacing="0px",n.style.width="max-content",e.style.width="max-content",e.style.wordSpacing="0px",e.style.letterSpacing="0px",e.style.width="max-content";const r=n.getBoundingClientRect().width||0,s=e.getBoundingClientRect().width||0,u=Math.max(r,s),c=this.affCountGaps(n.textContent||""),d=c>0?(u-r)/c:0,g=Math.max(Math.min(d,14),.5);this.affTargetWidth=Math.max(u,r+g*c),this.affCnLetterSpacing=g;const p=this.affTargetWidth,f=this.affCountEnSpaces(e.textContent||"");if(f>0)this.affEnWordSpacing=(p-s)/f,this.affEnLetterSpacing=0;else{const n=this.affCountEnGaps(e.textContent||"");this.affEnLetterSpacing=(p-s)/n,this.affEnWordSpacing=0}n.style.letterSpacing=l,n.style.width=t,e.style.width=a,e.style.wordSpacing=i,e.style.letterSpacing=o,e.style.wordSpacing=i,e.style.letterSpacing=o}}},B=l(6262);const W=(0,B.A)(_,[["render",F],["__scopeId","data-v-7dfce6ad"]]);var V=W;l(4114);const j={class:"top-row"},O=["href","title"],G=["title"],H={class:"tags-row"},K=["title"],z={class:"foot"},X=["title","aria-label"],Y=["title"],J=["href"],Z={key:2,class:"link-icon","aria-hidden":"true",title:"No external links"};var Q={__name:"PaperCard",props:{title:{type:String,required:!0},principleTag:{type:String,required:!0},stageTag:{type:String,required:!0},firstAuthor:{type:String,required:!0},link:{type:String,required:!0},fontScale:{type:Number,default:1.12},pubDate:{type:String,default:""}},setup(n){const e=n,l=(0,a.EW)(()=>{const n=(e.pubDate||"").toString().trim();if(!n)return"";const l=n.replace(/[\/-]/g,".");let t=l.match(/^(\d{4})\.(\d{1,2})$/);return t?`${t[1]}.${t[2].padStart(2,"0")}`:(t=l.match(/^(\d{2})\.(\d{1,2})$/),t?`20${t[1]}.${t[2].padStart(2,"0")}`:(t=l.match(/^(\d{4})(\d{2})$/),t?`${t[1]}.${t[2]}`:(t=l.match(/^(\d{2})(\d{2})$/),t?`20${t[1]}.${t[2]}`:l)))}),i=/[;,，、]\s*/g;function r(n){return String(n??"").replace(/[—–－]/g,"-").trim()}function s(n){return String(n||"").replace(/\u3000/g," ").replace(/[（）()]/g,"").replace(/(性|度)$/,"").trim()}const u={"安全-防滥用":"Safety - Abuse Prevention","安全-价值对齐":"Safety - Value Alignment","安全-抗攻击":"Safety - Attack Resistance","安全-隐私可保护":"Safety - Privacy Protection","安全-可标识":"Safety - Identifiability","可信-可解释":"Trustworthiness - Explainability","可信-可靠":"Trustworthiness - Reliability","可信-可控":"Trustworthiness - Controllability","可信-可审计":"Trustworthiness - Auditability","可信-准确":"Trustworthiness - Accuracy"},c={"安全":"Safety","可信":"Trustworthiness"},d={"防滥用":"Abuse Prevention","价值对齐":"Value Alignment","抗攻击":"Attack Resistance","隐私可保护":"Privacy Protection","可标识":"Identifiability","可解释":"Explainability","可靠":"Reliability","可控":"Controllability","可审计":"Auditability","准确":"Accuracy"},g={"指令理解":"Instruction Understanding","环境感知":"Environment Perception","物理交互":"Physical Interaction","决策规划":"Action Planning"};function p(n){const e=r(s(n));if(!e)return[];const l=e.split(i).filter(Boolean),t=[],a=new Set;for(const i of l)if(i.includes("-")){const[n,e]=i.split("-",2).map(s),l=`${n}-${e}`,o=u[l]||(c[n]?`${c[n]} - ${d[e]||e}`:l);a.has(o)||(a.add(o),t.push(o))}else{const n=d[i]||c[i]||i;a.has(n)||(a.add(n),t.push(n))}return t}function f(n){const e=s(n);if(!e)return[];const l=e.split(i).filter(Boolean),t=[],a=new Set;for(const i of l){const n=g[i]||i;a.has(n)||(a.add(n),t.push(n))}return t}const h=(0,a.EW)(()=>p(e.principleTag)),m=(0,a.EW)(()=>f(e.stageTag)),v=(0,a.EW)(()=>[...m.value.map(n=>({label:n,kind:"stage"})),...h.value.map(n=>({label:n,kind:"principle",tone:String(n).startsWith("Trustworthiness")?"trust":"safety"}))]),b=(0,a.EW)(()=>{const n=String(e.link||"").trim();return n?/^javascript:/i.test(n)?"#":/^https?:\/\//i.test(n)?n:n.startsWith("//")?"https:"+n:/^doi:/i.test(n)?"https://doi.org/"+n.replace(/^doi:\s*/i,""):/^10\.\d{4,9}\//.test(n)?"https://doi.org/"+n:/^arxiv:/i.test(n)?"https://arxiv.org/abs/"+n.replace(/^arxiv:\s*/i,""):/^\d{4}\.\d{4,5}(v\d+)?$/i.test(n)?"https://arxiv.org/abs/"+n:/^(mailto|ftp):/i.test(n)?n:/^[\w.-]+\.[a-z]{2,}([/:?#].*)?$/i.test(n)?"https://"+n:"#":"#"});return(e,i)=>((0,a.uX)(),(0,a.CE)("article",{class:"paper-card",style:(0,o.Tr)({"--pc-scale":String(n.fontScale)})},[(0,a.Lk)("div",j,[b.value&&"#"!==b.value?((0,a.uX)(),(0,a.CE)("a",{key:0,class:"title",href:b.value,target:"_blank",rel:"noopener noreferrer",title:n.title,onClick:i[0]||(i[0]=(0,t.D$)(()=>{},["stop"]))},(0,o.v_)(n.title),9,O)):((0,a.uX)(),(0,a.CE)("span",{key:1,class:"title disabled",title:n.title},(0,o.v_)(n.title),9,G)),(0,a.Lk)("div",H,[((0,a.uX)(!0),(0,a.CE)(a.FK,null,(0,a.pI)(v.value,(n,e)=>((0,a.uX)(),(0,a.CE)("span",{key:"tag-"+e,class:(0,o.C4)(["pill","stage"===n.kind?"pill-stage":"trust"===n.tone?"pill-trust":"pill-safety"]),title:n.label},(0,o.v_)(n.label),11,K))),128))])]),(0,a.Lk)("div",z,[(0,a.Lk)("span",{class:"author",title:`First author: ${n.firstAuthor}`,"aria-label":`First author: ${n.firstAuthor}`},(0,o.v_)(n.firstAuthor),9,X),l.value?((0,a.uX)(),(0,a.CE)("span",{key:0,class:"date-pill",title:l.value},(0,o.v_)(l.value),9,Y)):(0,a.Q3)("",!0),b.value&&"#"!==b.value?((0,a.uX)(),(0,a.CE)("a",{key:1,class:"link-icon",href:b.value,target:"_blank",rel:"noopener noreferrer","aria-label":"Open link",onClick:i[1]||(i[1]=(0,t.D$)(()=>{},["stop"]))},i[2]||(i[2]=[(0,a.Lk)("svg",{viewBox:"0 0 24 24",width:"16",height:"16","aria-hidden":"true"},[(0,a.Lk)("path",{d:"M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z",fill:"currentColor"})],-1)]),8,J)):((0,a.uX)(),(0,a.CE)("span",Z,i[3]||(i[3]=[(0,a.Lk)("svg",{viewBox:"0 0 24 24",width:"16",height:"16","aria-hidden":"true"},[(0,a.Lk)("path",{d:"M12 5v14M5 12h14",fill:"currentColor"})],-1)])))])],4))}};const q=(0,B.A)(Q,[["__scopeId","data-v-30882189"]]);var $=q;l(8237);const nn={class:"d-flex justify-center my-2"},en={ref:"turnstileEl",class:"cf-turnstile"},ln={class:"d-flex justify-center my-2"},tn={ref:"hcaptchaEl"},an={key:0,class:"my-2"},on={key:1,class:"text-error text-caption mt-2"},rn={key:2,class:"text-caption mt-2"};function sn(n,e,l,i,r,s){const u=(0,a.g2)("v-btn"),c=((0,a.g2)("v-tooltip"),(0,a.g2)("v-card-title")),d=(0,a.g2)("v-text-field"),g=(0,a.g2)("v-card-text"),p=(0,a.g2)("v-card-actions"),f=(0,a.g2)("v-card"),h=(0,a.g2)("v-dialog");return(0,a.uX)(),(0,a.CE)(a.FK,null,[(0,a.Q3)("",!0),(0,a.Q3)("",!0),(0,a.bF)(h,{modelValue:i.showVerify,"onUpdate:modelValue":e[3]||(e[3]=n=>i.showVerify=n),"max-width":"440",persistent:""},{default:(0,a.k6)(()=>[(0,a.bF)(f,null,{default:(0,a.k6)(()=>[(0,a.bF)(c,{class:"text-h6"},{default:(0,a.k6)(()=>e[4]||(e[4]=[(0,a.eW)("Verification")])),_:1,__:[4]}),(0,a.bF)(g,null,{default:(0,a.k6)(()=>[e[6]||(e[6]=(0,a.Lk)("div",{class:"text-body-2 mb-3"}," Please complete a quick check so we know you're human. ",-1)),(0,a.bo)((0,a.Lk)("div",nn,[(0,a.Lk)("div",en,null,512)],512),[[t.aG,"turnstile"===i.mode]]),(0,a.bo)((0,a.Lk)("div",ln,[(0,a.Lk)("div",tn,null,512)],512),[[t.aG,"hcaptcha"===i.mode]]),"basic"===i.mode?((0,a.uX)(),(0,a.CE)("div",an,[e[5]||(e[5]=(0,a.Lk)("div",{class:"text-body-2 mb-3"},"Quick math check:",-1)),(0,a.bF)(d,{modelValue:i.basicAnswer,"onUpdate:modelValue":e[2]||(e[2]=n=>i.basicAnswer=n),modelModifiers:{trim:!0},label:i.basicQuestion,type:"number","hide-details":"",density:"comfortable"},null,8,["modelValue","label"])])):(0,a.Q3)("",!0),i.errorMsg?((0,a.uX)(),(0,a.CE)("div",on,(0,o.v_)(i.errorMsg),1)):(0,a.Q3)("",!0),i.hintMsg?((0,a.uX)(),(0,a.CE)("div",rn,(0,o.v_)(i.hintMsg),1)):(0,a.Q3)("",!0)]),_:1,__:[6]}),(0,a.bF)(p,{class:"justify-end"},{default:(0,a.k6)(()=>[(0,a.bF)(u,{variant:"text",onClick:i.closeVerify,disabled:i.verifying},{default:(0,a.k6)(()=>e[7]||(e[7]=[(0,a.eW)("Cancel")])),_:1,__:[7]},8,["onClick","disabled"]),(0,a.bF)(u,{variant:"tonal",color:"primary",loading:i.verifying,disabled:!i.canSubmit,onClick:i.submitOrRefresh},{default:(0,a.k6)(()=>[(0,a.eW)((0,o.v_)(i.actionLabel),1)]),_:1},8,["loading","disabled","onClick"])]),_:1})]),_:1})]),_:1},8,["modelValue"])],64)}var un=l(953),cn={name:"SubmitPaperClient",props:{buttonLabel:{type:String,default:"SHARE YOUR IDEA ✨"},variant:{type:String,default:"outlined"},color:{type:String,default:"primary"},rounded:{type:[Boolean,String,Number],default:"xl"},elevation:{type:[String,Number],default:2},size:{type:String,default:"default"},density:{type:String,default:"comfortable"},buttonHeight:{type:[String,Number],default:56},tooltip:{type:String,default:"Think your work fits this topic? Share it with us — we review regularly."},tooltipLocation:{type:String,default:"left"},href:{type:String,default:""},gfView:{type:String,default:""},gfAction:{type:String,default:""},openInNewTab:{type:Boolean,default:!0},redirectDelay:{type:Number,default:0},fixed:{type:Boolean,default:!0},align:{type:String,default:"right"},offsetX:{type:Number,default:24},offsetY:{type:Number,default:24},zIndex:{type:Number,default:2140},reserveBelow:{type:Number,default:160},preferred:{type:String,default:"basic"},turnstileSiteKey:{type:String,default:""},hcaptchaSiteKey:{type:String,default:""},verifyEndpoint:{type:String,default:""},verifyMethod:{type:String,default:"POST"},verifyWithCreds:{type:Boolean,default:!1}},setup(n){const e=(0,a.EW)(()=>{if(n.href)return n.href;if(n.gfView)return n.gfView;if(n.gfAction)try{return String(n.gfAction).replace("/formResponse","/viewform")}catch{}return"https://docs.google.com/forms/d/1LdZXCC7ufWNOgulFCSBNwlx0mXTF73tQ38Uyq-MxykQ"}),l="https://docs.google.com/forms/d/1ucW4QVJ77C7z9qEn34bsy_2YIYbF52z5l8Q-gsUKn6s",t="MEET SOME TROUBLES",i=(0,a.EW)(()=>{const e={position:"fixed",zIndex:String(n.zIndex)},l=n.offsetY+n.reserveBelow;return"left"===n.align?e.left=n.offsetX+"px":e.right=n.offsetX+"px",e.bottom=l+"px",e}),o=(0,un.KR)(!1),r=(0,un.KR)(n.preferred||"basic"),s=(0,un.KR)(null),u=(0,un.KR)(null),c=(0,un.KR)(null),d=(0,un.KR)(!1),g=(0,un.KR)(!1),p=(0,un.KR)(""),f=(0,un.KR)(""),h=Math.floor(10+40*Math.random()),m=Math.floor(10+40*Math.random()),v=`What is ${h} + ${m}?`,b=(0,un.KR)(""),y=(0,un.KR)(""),A="spc_verified_once",S=2592e6;function L(){try{const n=localStorage.getItem(A);if(!n)return!1;const e=JSON.parse(n).t||0;return Date.now()-e<S}catch{return!1}}function k(){try{localStorage.setItem(A,JSON.stringify({t:Date.now()}))}catch{}}function w(){L()?T():(p.value="",f.value="",d.value=!1,o.value=!0,r.value=n.preferred||"basic",(0,a.dY)(()=>P()))}function M(n){y.value=n||"",L()?T():w()}function E(){o.value=!1,I()}const x=(0,a.EW)(()=>"basic"===r.value?String(Number(h+m))===String(b.value||""):d.value&&!g.value),C=(0,a.EW)(()=>g.value?"Verifying...":"Verify & Continue");async function R(){if(x.value){g.value=!0;try{T()}catch(n){p.value="Verification error. Please try again."}finally{g.value=!1}}}function T(){setTimeout(()=>{k();const l=n.openInNewTab?"_blank":"_self",t=y.value||e.value;window.open(t,l,"noopener"),y.value=""},n.redirectDelay||0),o.value=!1,I()}function I(){try{c.value&&window.hcaptcha&&window.hcaptcha.reset(c.value)}catch{}c.value=null,d.value=!1}async function P(){if("turnstile"===r.value)try{await D(),d.value=!0,f.value=""}catch{r.value="basic"}else if("hcaptcha"===r.value)try{await F(),d.value=!0,f.value=""}catch{r.value="basic"}else d.value=!0,f.value=""}function D(){return new Promise((n,e)=>{if("undefined"===typeof window)return e();if(window.turnstile)return n();const l="cf-turnstile-script";if(document.getElementById(l))return n();const t=document.createElement("script");t.id=l,t.src="https://challenges.cloudflare.com/turnstile/v0/api.js",t.async=!0,t.defer=!0,t.onload=()=>n(),t.onerror=()=>e(new Error("turnstile load error")),document.head.appendChild(t),setTimeout(()=>{window.turnstile||e(new Error("turnstile timeout"))},6e3)})}function F(){return new Promise((n,e)=>{if("undefined"===typeof window)return e();if(window.hcaptcha)return n();const l="hcaptcha-script";if(document.getElementById(l))return n();const t=document.createElement("script");t.id=l,t.src="https://js.hcaptcha.com/1/api.js",t.async=!0,t.defer=!0,t.onload=()=>n(),t.onerror=()=>e(new Error("hcaptcha load error")),document.head.appendChild(t),setTimeout(()=>{window.hcaptcha||e(new Error("hcaptcha timeout"))},6e3)})}return{viewUrl:e,helpUrl:l,helpLabel:t,fixedStyle:i,showVerify:o,mode:r,turnstileEl:s,hcaptchaEl:u,widgetReady:d,verifying:g,errorMsg:p,hintMsg:f,basicQuestion:v,basicAnswer:b,openVerify:w,openVerifyTo:M,closeVerify:E,submitOrRefresh:R,canSubmit:x,actionLabel:C}}};const dn=(0,B.A)(cn,[["render",sn],["__scopeId","data-v-2a4cd25c"]]);var gn=dn;const pn={class:"tmx__toolbar"},fn={class:"tmx__left-actions"},hn={class:"tmx__cta-right"},mn={class:"tmx__matrixWrap"},vn=["title"],bn={class:"rowhead"},yn={class:"rowhead__text"},An=["title","onClick"],Sn={class:"count"},Ln={key:0,class:"chips"},kn=["onClick"],wn={class:"tmx__summary-row"},Mn={class:"tmx__summary"},En={class:"tmx__summary-right"},xn="__@@__";var Cn={__name:"TagMatrix",props:{fitPage:{type:Boolean,default:!0},colMin:{type:Number,default:112},rowLabelWidth:{type:Number,default:168},rows:{type:Array,default:()=>[]},rowsData:{type:Array,default:()=>[]},placeholder:{type:String,default:"Search papers, authors, tags…"},showSearch:{type:Boolean,default:!1},onTagClick:{type:Function,default:null}},emits:["filtered"],setup(n,{emit:e}){const l=n,t=e,i=(0,a.EW)(()=>l.fitPage?"full":""),r=(0,a.EW)(()=>({"--tmx-col-min":l.colMin+"px","--tmx-row-label-w":l.rowLabelWidth+"px"})),s=(0,a.EW)(()=>l.rows&&l.rows.length?l.rows:l.rowsData||[]),u={"安全":"Safety","可信":"Trustworthiness"},c={"防滥用":"Abuse Prevention","价值对齐":"Value Alignment","抗攻击":"Attack Resistance","隐私可保护":"Privacy Protection","可标识":"Identifiability","可解释":"Explainability","可靠":"Reliability","可控":"Controllability","可审计":"Auditability","准确":"Accuracy"},d={"指令理解":"Instruction Understanding","环境感知":"Environment Perception","物理交互":"Physical Interaction","决策规划":"Action Planning"},g=[{cn:"安全-防滥用",en:"Safety - Abuse Prevention",kind:"safety"},{cn:"安全-价值对齐",en:"Safety - Value Alignment",kind:"safety"},{cn:"安全-抗攻击",en:"Safety - Attack Resistance",kind:"safety"},{cn:"安全-隐私可保护",en:"Safety - Privacy Protection",kind:"safety"},{cn:"安全-可标识",en:"Safety - Identifiability",kind:"safety"},{cn:"可信-可解释",en:"Trustworthiness - Explainability",kind:"trust"},{cn:"可信-可靠",en:"Trustworthiness - Reliability",kind:"trust"},{cn:"可信-可控",en:"Trustworthiness - Controllability",kind:"trust"},{cn:"可信-可审计",en:"Trustworthiness - Auditability",kind:"trust"},{cn:"可信-准确",en:"Trustworthiness - Accuracy",kind:"trust"}],p=[{cn:"指令理解",en:"Instruction Understanding"},{cn:"环境感知",en:"Environment Perception"},{cn:"物理交互",en:"Physical Interaction"},{cn:"决策规划",en:"Action Planning"}];function f(n){return String(n).replace(/^\s*Safety\s*-\s*/i,"").replace(/^\s*Trustworthiness\s*-\s*/i,"").trim()}const h=/[;,，、]\s*/g;function m(n){return String(n||"").replace(/\u3000/g," ").replace(/[（）()]/g,"").replace(/(性|度)$/,"").trim()}function v(n){const e=m(n);if(!e)return[];const l=e.split(h).filter(Boolean),t=[],a=new Set;for(const i of l){i.includes("-")||u[i];const n=i;a.has(n)||(a.add(n),t.push(n))}return t}function b(n){const e=m(n);return e?e.split(h).filter(Boolean):[]}function y(n){const e=m(n);if(!e)return"";if(e.includes("-")){const[n,l]=e.split("-",2),t=u[n]||n,a=c[l]||l;return`${t} - ${a}`}return c[e]||u[e]||e}function A(n){return d[n]||n}const S=(0,un.KR)("");function L(n,e){return`${n}${xn}${e}`}const k=(0,un.Kh)(new Set),w=(0,a.EW)(()=>(s.value||[]).map(n=>{const e=n.title||n["标题"]||n["题目"]||"",l=n.link||n.url||n.href||n["外链"]||n["链接"]||n["论文链接"]||n["原文链接"]||n["地址"]||n["source"]||n["Source"]||n["paperUrl"]||n["pdf"]||n["arXiv"]||"",t=n.firstAuthor||n["一作"]||n["第一作者"]||"",a=n.date||n["发表年月"]||n["发表时间"]||"",i=n.principleTag||n["10大原则"]||n["原则"]||"",o=n.stageTag||n["4个阶段"]||n["阶段"]||"",r=v(i),s=b(o),u=r.map(y).filter(Boolean),c=s.map(A).filter(Boolean);return{raw:n,title:e,link:l,firstAuthor:t,date:a,principleEnList:u,stageEnList:c,haystack:[e,t,u.join(" "),c.join(" ")].join(" ").toLowerCase()}})),M=(0,a.EW)(()=>{const n=S.value.trim().toLowerCase(),e=w.value.filter(e=>!n||e.haystack.includes(n));return k.size?e.filter(n=>{for(const e of k){const[l,t]=e.split(xn);if(n.stageEnList.includes(l)&&n.principleEnList.includes(t))return!0}return!1}):e});function E(n,e){return w.value.reduce((l,t)=>l+(t.stageEnList.includes(n)&&t.principleEnList.includes(e)?1:0),0)}function x(n,e){return k.has(L(n,e))}function C(n,e){const l=L(n,e);k.has(l)?k.delete(l):k.add(l)}function R(n,e){const l=E(n,e);return`${n} × ${e} — ${l} paper${1===l?"":"s"}`}(0,a.wB)(M,n=>t("filtered",n.map(n=>({...n.raw,link:n.raw?.link||n.link||n.raw?.url||n.raw?.href||n.raw?.["外链"]||n.raw?.["链接"]||n.raw?.["论文链接"]||n.raw?.["原文链接"]||n.raw?.["地址"]||n.raw?.source||n.raw?.Source||n.raw?.paperUrl||n.raw?.pdf||n.raw?.arXiv||""}))),{immediate:!0});const T=(0,a.EW)(()=>{const n=[];for(const e of k){const[l,t]=e.split(xn);n.push({key:e,rowEn:l,colEn:t})}return n});function I(n){k.delete(n.key)}function P(){k.clear(),S.value=""}return(n,e)=>((0,a.uX)(),(0,a.CE)("section",{class:"tmx tmx--compact",style:(0,o.Tr)(r.value)},[(0,a.Lk)("div",pn,[e[0]||(e[0]=(0,a.Lk)("div",{class:"tmx__title"},[(0,a.Lk)("span",{class:"bar","aria-hidden":"true"}),(0,a.Lk)("h2",null,"Filter by Categories")],-1)),(0,a.Lk)("div",fn,[(0,a.RG)(n.$slots,"left-actions")]),e[1]||(e[1]=(0,a.Lk)("div",{class:"tmx__spacer"},null,-1)),(0,a.Lk)("div",hn,[(0,a.RG)(n.$slots,"actions"),(0,a.bF)(gn,{buttonLabel:"Share your paper ✨",showHint:!1,variant:"outlined",color:"primary",rounded:"pill",density:"comfortable",size:"default",prependIcon:""})])]),e[5]||(e[5]=(0,a.Lk)("div",{class:"tmx__legend"},[(0,a.Lk)("span",{class:"dot safety"}),(0,a.Lk)("span",null,"Safety"),(0,a.Lk)("span",{class:"separator"},"•"),(0,a.Lk)("span",{class:"dot trust"}),(0,a.Lk)("span",null,"Trustworthiness")],-1)),(0,a.Lk)("div",mn,[(0,a.Lk)("div",{class:(0,o.C4)(["tmx__matrix",i.value])},[e[2]||(e[2]=(0,a.Lk)("div",{class:"corner"},null,-1)),((0,a.uX)(),(0,a.CE)(a.FK,null,(0,a.pI)(g,n=>(0,a.Lk)("div",{key:"col-"+n.en,class:(0,o.C4)(["colhead",n.kind])},[(0,a.Lk)("span",{class:"colhead__text",title:n.en},(0,o.v_)(f(n.en)),9,vn)],2)),64)),((0,a.uX)(),(0,a.CE)(a.FK,null,(0,a.pI)(p,n=>((0,a.uX)(),(0,a.CE)(a.FK,{key:"row-"+n.en},[(0,a.Lk)("div",bn,[(0,a.Lk)("span",yn,(0,o.v_)(n.en),1)]),((0,a.uX)(),(0,a.CE)(a.FK,null,(0,a.pI)(g,e=>(0,a.Lk)("button",{key:n.en+"-"+e.en,class:(0,o.C4)(["cell",{active:x(n.en,e.en),safety:"safety"===e.kind,trust:"trust"===e.kind}]),title:R(n.en,e.en),onClick:l=>C(n.en,e.en)},[(0,a.Lk)("span",Sn,(0,o.v_)(E(n.en,e.en)),1)],10,An)),64))],64))),64))],2)]),T.value.length?((0,a.uX)(),(0,a.CE)("div",Ln,[((0,a.uX)(!0),(0,a.CE)(a.FK,null,(0,a.pI)(T.value,n=>((0,a.uX)(),(0,a.CE)("button",{class:"chip",key:n.key,onClick:e=>I(n)},[(0,a.eW)((0,o.v_)(n.rowEn)+" × "+(0,o.v_)(f(n.colEn))+" ",1),e[3]||(e[3]=(0,a.Lk)("span",{class:"x"},"✕",-1))],8,kn))),128)),(0,a.Lk)("button",{class:"chip",onClick:P},"Clear all")])):(0,a.Q3)("",!0),(0,a.Lk)("div",wn,[(0,a.Lk)("div",Mn,[(0,a.Lk)("strong",null,(0,o.v_)(M.value.length),1),e[4]||(e[4]=(0,a.eW)(" paper(s) match current filters"))]),(0,a.Lk)("div",En,[(0,a.RG)(n.$slots,"footer-right")])])],4))}};const Rn=(0,B.A)(Cn,[["__scopeId","data-v-965f9f00"]]);var Tn=Rn,In=l(3562),Pn=l.n(In);console.info("[paperSearch] strict-phrase-fuzzy-all-v2 loaded");const Dn=n=>(n||"").toLowerCase().normalize("NFKC").replace(/[_/]+/g," ").replace(/[^\p{L}\p{N}\s-]/gu," ").replace(/-+/g," ").replace(/\s+/g," ").trim(),Fn=n=>Dn(n).split(" ").filter(Boolean);function Nn(n,e){const l=n.length,t=e.length;if(!l)return t;if(!t)return l;const a=new Array(t+1);for(let i=0;i<=t;i++)a[i]=i;for(let i=1;i<=l;i++){let l=a[0];a[0]=i;for(let o=1;o<=t;o++){const t=a[o];n[i-1]===e[o-1]?a[o]=l:a[o]=Math.min(l+1,a[o]+1,a[o-1]+1),l=t}}return a[t]}function Un(n,e,l){if(!n||!e)return{ok:!1,sim:0};const t=n.length;if(l&&e.startsWith(n)){const l=n.length/Math.max(n.length,e.length),a=t>=7?.7:t>=5?.6:4===t?.5:3===t?.375:2===t?.25:.1;if(l>=a)return{ok:!0,sim:l}}if(t<=2)return{ok:n===e,sim:n===e?1:0};const a=t>=8?2:1,i=Nn(n,e);if(i>a)return{ok:!1,sim:0};const o=1-i/Math.max(t,e.length),r=t<=4?.75:.8;return{ok:o>=r,sim:o}}function _n(n,e){if(!e.length)return{ok:!0,sim:1,start:-1};const l=n.length,t=e.length;let a=-1,i=-1;for(let o=0;o+t<=l;o++){let l=0,r=!0;for(let a=0;a<t;a++){const{ok:i,sim:s}=Un(e[a],n[o+a],a===t-1);if(!i){r=!1;break}l+=s}r&&l>a&&(a=l,i=o)}return a>=0?{ok:!0,sim:a/t,start:i}:{ok:!1,sim:0,start:-1}}function Bn(n,e,l){const t=n.length;let a=-1;for(let i=0;i<t-1;i++){const t=Un(e,n[i],!1),o=Un(l,n[i+1],!0);t.ok&&o.ok&&(a=Math.max(a,(t.sim+o.sim)/2))}return a>=0?{ok:!0,sim:a}:{ok:!1,sim:0}}function Wn(n){const e=[['"','"'],["“","”"],["「","」"],["『","』"],["＂","＂"]];for(const[l,t]of e){const e=n.indexOf(l),a=n.indexOf(t,e+1);if(e>=0&&a>e)return n.slice(e+1,a)}return""}function Vn(n,e,l=60){const t=(e||"").trim();if(!t)return[];const a=Wn(t),i=Fn(a),o=Dn(a?t.replace(a,"").replace(/["“”「」『』＂]/g,""):t),r=Fn(o),s=r.length,u=[];for(const d of n){const n=Dn(d.title||"");if(!n)continue;const e=Fn(n);let l=!1,t=0;if(i.length){const n=_n(e,i);if(!n.ok)continue;l=!0,t+=1e3+100*n.sim}if(!l)if(s>=3){const n=_n(e,r);if(!n.ok)continue;l=!0,t+=800+100*n.sim}else if(2===s){const n=Bn(e,r[0],r[1]);if(!n.ok)continue;l=!0,t+=700+100*n.sim}else{if(1!==s)continue;{let n=-1;for(let l=0;l<e.length;l++){const t=Un(r[0],e[l],!0);t.ok&&(n=Math.max(n,t.sim))}if(n<0)continue;l=!0,t+=200+100*n}}u.push({item:d,score:t})}u.sort((n,e)=>e.score-n.score||(n.item.title||"").localeCompare(e.item.title||""));const c=l>0?u.slice(0,l):u;return c.map(n=>n.item)}const jn={class:"mt-8"},On={class:"bibtex-header"},Gn={class:"bibtex-box"},Hn={class:"bibtex-pre"};var Kn={__name:"BibTeXSection",props:{type:{type:String,default:"misc"},keyId:{type:String,default:"your_key_here"},title:{type:String,default:"<< Put Your Paper Title Here >>"},authors:{type:Array,default:()=>["Xin Tan","Bangwei Liu","Yicheng Bao","Qijian Tian","Zhenkun Gao","Xiongbin Wu","Zhihao Luo","Sen Wang","Yuqi Zhang","Xuhong Wang","Chaochao Lu","Bowen Zhou"]},year:{type:[String,Number],default:"2025"},eprint:{type:String,default:"2501.xxxxx"},archivePrefix:{type:String,default:"arXiv"},primaryClass:{type:String,default:"cs.LG"},url:{type:String,default:"https://arxiv.org/abs/2501.xxxxx"}},setup(n){const e=n,l=(0,a.EW)(()=>`@${e.type}{${e.keyId},\n  title={${e.title}},\n  author={${e.authors.join(" and ")}},\n  year={${e.year}},\n  eprint={${e.eprint}},\n  archivePrefix={${e.archivePrefix}},\n  primaryClass={${e.primaryClass}},\n  url={${e.url}},\n}`),t=(0,un.KR)(!1),i=async()=>{try{t.value=!0,await navigator.clipboard.writeText(l.value)}finally{setTimeout(()=>t.value=!1,700)}};return(n,e)=>{const r=(0,a.g2)("v-btn");return(0,a.uX)(),(0,a.CE)("section",jn,[(0,a.Lk)("div",On,[e[0]||(e[0]=(0,a.Lk)("h2",{class:"bibtex-title"},"BibTeX",-1)),(0,a.bF)(r,{size:"small",variant:"text",onClick:i,loading:t.value},{default:(0,a.k6)(()=>[(0,a.eW)((0,o.v_)(t.value?"Copied":"Copy"),1)]),_:1},8,["loading"])]),(0,a.Lk)("div",Gn,[(0,a.Lk)("pre",Hn,[(0,a.Lk)("code",null,(0,o.v_)(l.value),1)])])])}}};const zn=(0,B.A)(Kn,[["__scopeId","data-v-20962801"]]);var Xn=zn;const Yn={key:1,class:"spc-inline"},Jn={class:"d-flex justify-center my-2"},Zn={key:0,class:"text-error text-caption mt-2"};function Qn(n,e,l,i,r,s){const u=(0,a.g2)("ImpactMetrics"),c=(0,a.g2)("v-btn"),d=(0,a.g2)("v-tooltip"),g=(0,a.g2)("v-card-title"),p=(0,a.g2)("v-text-field"),f=(0,a.g2)("v-card-text"),h=(0,a.g2)("v-card-actions"),m=(0,a.g2)("v-card"),v=(0,a.g2)("v-dialog");return(0,a.uX)(),(0,a.CE)(a.FK,null,[l.fixed?((0,a.uX)(),(0,a.Wv)(a.Im,{key:0,to:"body"},[(0,a.Lk)("div",{class:"spc-fixed",style:(0,o.Tr)(i.fixedStyle)},[l.showImpact?((0,a.uX)(),(0,a.Wv)(u,{key:0,class:"mb-2","stars-repo":l.starsRepo,"updated-repo":l.updatedRepo,"visitors-api":l.visitorsApi,"collect-api":l.collectApi,"github-token":l.githubToken,"cta-text":l.impactCtaText},null,8,["stars-repo","updated-repo","visitors-api","collect-api","github-token","cta-text"])):(0,a.Q3)("",!0),l.tooltip?((0,a.uX)(),(0,a.Wv)(d,{key:1,text:l.tooltip,location:l.tooltipLocation,"open-on-hover":""},{activator:(0,a.k6)(({props:n})=>[(0,a.bF)(c,(0,a.v6)(n,{class:"spc-btn",variant:l.variant,color:l.color,rounded:l.rounded,elevation:0,size:l.size,density:l.density,height:l.buttonHeight,onClick:i.openVerify}),{default:(0,a.k6)(()=>[(0,a.eW)((0,o.v_)(l.buttonLabel),1)]),_:2},1040,["variant","color","rounded","size","density","height","onClick"])]),_:1},8,["text","location"])):((0,a.uX)(),(0,a.Wv)(c,{key:2,class:"spc-btn",variant:l.variant,color:l.color,rounded:l.rounded,elevation:0,size:l.size,density:l.density,height:l.buttonHeight,onClick:i.openVerify},{default:(0,a.k6)(()=>[(0,a.eW)((0,o.v_)(l.buttonLabel),1)]),_:1},8,["variant","color","rounded","size","density","height","onClick"])),(0,a.bF)(c,{class:"spc-btn",variant:l.variant,color:l.color,rounded:l.rounded,elevation:0,size:l.size,density:l.density,height:l.buttonHeight,onClick:e[0]||(e[0]=n=>i.openVerifyTo(i.helpUrl))},{default:(0,a.k6)(()=>[(0,a.eW)((0,o.v_)(l.helpLabel),1)]),_:1},8,["variant","color","rounded","size","density","height"])],4)])):((0,a.uX)(),(0,a.CE)("div",Yn,[(0,a.bF)(c,{class:"spc-btn",variant:l.variant,color:l.color,rounded:l.rounded,elevation:0,size:l.size,density:l.density,height:l.buttonHeight,onClick:i.openVerify},{default:(0,a.k6)(()=>[(0,a.eW)((0,o.v_)(l.buttonLabel),1)]),_:1},8,["variant","color","rounded","size","density","height","onClick"]),(0,a.bF)(c,{class:"spc-btn",variant:l.variant,color:l.color,rounded:l.rounded,elevation:0,size:l.size,density:l.density,height:l.buttonHeight,onClick:e[1]||(e[1]=n=>i.openVerifyTo(i.helpUrl))},{default:(0,a.k6)(()=>[(0,a.eW)((0,o.v_)(l.helpLabel),1)]),_:1},8,["variant","color","rounded","size","density","height"])])),(0,a.bF)(v,{modelValue:i.showVerify,"onUpdate:modelValue":e[3]||(e[3]=n=>i.showVerify=n),"max-width":"440",persistent:""},{default:(0,a.k6)(()=>[(0,a.bF)(m,null,{default:(0,a.k6)(()=>[(0,a.bF)(g,{class:"text-h6"},{default:(0,a.k6)(()=>e[4]||(e[4]=[(0,a.eW)("Verification")])),_:1,__:[4]}),(0,a.bF)(f,null,{default:(0,a.k6)(()=>[e[5]||(e[5]=(0,a.Lk)("div",{class:"text-body-2 mb-3"},"Please complete a quick check so we know you're human.",-1)),(0,a.bo)((0,a.Lk)("div",Jn,[(0,a.bF)(p,{modelValue:i.basicAnswer,"onUpdate:modelValue":e[2]||(e[2]=n=>i.basicAnswer=n),modelModifiers:{trim:!0},label:i.basicQuestion,type:"number","hide-details":"",density:"comfortable"},null,8,["modelValue","label"])],512),[[t.aG,"basic"===n.mode]]),i.errorMsg?((0,a.uX)(),(0,a.CE)("div",Zn,(0,o.v_)(i.errorMsg),1)):(0,a.Q3)("",!0)]),_:1,__:[5]}),(0,a.bF)(h,{class:"justify-end"},{default:(0,a.k6)(()=>[(0,a.bF)(c,{variant:"text",onClick:i.closeVerify,disabled:i.verifying},{default:(0,a.k6)(()=>e[6]||(e[6]=[(0,a.eW)("Cancel")])),_:1,__:[6]},8,["onClick","disabled"]),(0,a.bF)(c,{variant:"tonal",color:"primary",loading:i.verifying,disabled:!i.canSubmit,onClick:i.submitOrRefresh},{default:(0,a.k6)(()=>[(0,a.eW)((0,o.v_)(i.actionLabel),1)]),_:1},8,["loading","disabled","onClick"])]),_:1})]),_:1})]),_:1},8,["modelValue"])],64)}const qn={class:"metrics"},$n={class:"metric"},ne={class:"value"},ee={class:"metric"},le={class:"value"},te={key:0,class:"countries"},ae={class:"countries-title"},ie={class:"country-list"},oe={class:"flag"},re={class:"name"},se={class:"count"},ue={class:"cta"};function ce(n,e,l,i,r,s){return(0,a.uX)(),(0,a.Wv)(a.Im,{to:"body"},[i.hidden?((0,a.uX)(),(0,a.CE)("button",{key:0,class:"metrics-fab",onClick:e[0]||(e[0]=(0,t.D$)((...n)=>i.onReopen&&i.onReopen(...n),["stop"])),"aria-label":"Open metrics"},"ⓘ")):((0,a.uX)(),(0,a.CE)("div",{key:1,class:"impact-card floating",style:(0,o.Tr)({left:i.pos.x+"px",bottom:i.pos.y+"px"}),onMousedown:e[2]||(e[2]=(...n)=>i.onDragStart&&i.onDragStart(...n)),onTouchstart:e[3]||(e[3]=(0,t.D$)((...n)=>i.onDragStart&&i.onDragStart(...n),["prevent"]))},[(0,a.Lk)("button",{class:"close","aria-label":"Close",onClick:e[1]||(e[1]=(0,t.D$)((...n)=>i.onClose&&i.onClose(...n),["stop"]))},"×"),(0,a.Lk)("div",qn,[(0,a.Lk)("div",$n,[e[4]||(e[4]=(0,a.Lk)("div",{class:"label"},"GitHub Stars",-1)),(0,a.Lk)("div",ne,(0,o.v_)(i.formatNumber(i.stars)),1)]),(0,a.Lk)("div",ee,[e[5]||(e[5]=(0,a.Lk)("div",{class:"label"},"Last Updated",-1)),(0,a.Lk)("div",le,(0,o.v_)(i.updatedAt||"—"),1)])]),i.countries.length?((0,a.uX)(),(0,a.CE)("div",te,[(0,a.Lk)("div",ae,"来访国家统计（Top "+(0,o.v_)(i.topN)+"）",1),(0,a.Lk)("div",ie,[((0,a.uX)(!0),(0,a.CE)(a.FK,null,(0,a.pI)(i.countries,n=>((0,a.uX)(),(0,a.CE)("div",{key:n.code,class:"country"},[(0,a.Lk)("span",oe,(0,o.v_)(i.flagEmoji(n.code)),1),(0,a.Lk)("span",re,(0,o.v_)(i.regionName(n.code)),1),(0,a.Lk)("span",se,(0,o.v_)(i.formatNumber(n.count)),1)]))),128))])])):(0,a.Q3)("",!0),(0,a.Lk)("div",ue,(0,o.v_)(l.ctaText),1)],36))])}const de="IMPACT_METRICS_POS",ge="IMPACT_METRICS_HIDE",pe="IMPACT_METRICS_STARS",fe="IMPACT_METRICS_UPDATED";var he={name:"ImpactMetrics",props:{starsRepo:{type:String,default:""},updatedRepo:{type:String,default:""},githubToken:{type:String,default:""},visitorsApi:{type:String,default:""},collectApi:{type:String,default:""},topN:{type:Number,default:5},ctaText:{type:String,default:""}},setup(n){const e=(0,un.KR)(null),l=(0,un.KR)(""),t=(0,un.KR)([]),i=(0,un.KR)(!1),o=(0,un.KR)({x:16,y:16});try{const n=localStorage.getItem(pe);n&&(e.value=Number(n));const t=localStorage.getItem(fe);t&&(l.value=t)}catch{}const r=(n,e,l)=>Math.min(Math.max(n,e),l),s=(0,un.KR)({x:0,y:0}),u=(0,un.KR)({x:16,y:16}),c=(0,un.KR)(!1);function d(n){c.value=!0;const e=n.touches?n.touches[0]:n;s.value={x:e.clientX,y:e.clientY},u.value={...o.value},window.addEventListener("mousemove",g),window.addEventListener("mouseup",p),window.addEventListener("touchmove",g,{passive:!1}),window.addEventListener("touchend",p)}function g(n){if(!c.value)return;const e=n.touches?n.touches[0]:n,l=e.clientX-s.value.x,t=e.clientY-s.value.y,a=window.innerWidth,i=window.innerHeight,d=document.querySelector(".impact-card"),g=d?d.offsetWidth:210,p=d?d.offsetHeight:160;o.value.x=r(u.value.x+l,8,Math.max(8,a-g-8)),o.value.y=r(u.value.y-t,8,Math.max(8,i-p-8)),n.preventDefault?.()}function p(){if(c.value){c.value=!1;try{localStorage.setItem(de,JSON.stringify(o.value))}catch{}window.removeEventListener("mousemove",g),window.removeEventListener("mouseup",p),window.removeEventListener("touchmove",g),window.removeEventListener("touchend",p)}}function f(){i.value=!0;try{localStorage.setItem(ge,"1")}catch{}}function h(){i.value=!1;try{localStorage.removeItem(ge)}catch{}}function m(){try{const n=JSON.parse(localStorage.getItem(de)||"null");n&&"number"===typeof n.x&&"number"===typeof n.y&&(o.value=n)}catch{}try{i.value="1"===localStorage.getItem(ge)}catch{}}function v(n){return null==n?"—":n>=1e6?(n/1e6).toFixed(1)+"m":n>=1e3?(n/1e3).toFixed(1)+"k":String(n)}function b(n){try{const e=new Intl.DisplayNames([navigator.language||"zh-CN"],{type:"region"});return e.of(n)||n}catch{return n}}function y(n){if(!n||2!==n.length)return"🏳️";const e=n.toUpperCase(),l=127462;return String.fromCodePoint(l+e.charCodeAt(0)-65)+String.fromCodePoint(l+e.charCodeAt(1)-65)}async function A(e){const l={Accept:"application/vnd.github+json","X-GitHub-Api-Version":"2022-11-28"};try{const n=await fetch(e,{headers:l,mode:"cors"});if(n.ok)return await n.json()}catch{}if(n.githubToken)for(const t of[`Bearer ${n.githubToken}`,`token ${n.githubToken}`])try{const n=await fetch(e,{headers:{...l,Authorization:t},mode:"cors"});if(n.ok)return await n.json()}catch{}return null}async function S(){if(n.starsRepo)try{const l=await A(`https://api.github.com/repos/${n.starsRepo}`);if(l&&null!=l.stargazers_count){e.value=l.stargazers_count;try{localStorage.setItem(pe,String(e.value))}catch{}}}catch{}}async function L(){if(n.updatedRepo)try{const e=await A(`https://api.github.com/repos/${n.updatedRepo}`),t=e&&(e.pushed_at||e.updated_at||e.created_at);if(t){const n=new Date(t),e=n.getFullYear(),a=String(n.getMonth()+1).padStart(2,"0"),i=String(n.getDate()).padStart(2,"0"),o=String(n.getHours()).padStart(2,"0"),r=String(n.getMinutes()).padStart(2,"0");l.value=`${e}-${a}-${i} ${o}:${r}`;try{localStorage.setItem(fe,l.value)}catch{}}}catch{}}async function k(){if(n.visitorsApi)try{const e=await fetch(n.visitorsApi,{cache:"no-store"});if(!e.ok)throw 0;const l=await e.json();let a=[];Array.isArray(l)?a=l:Array.isArray(l.countries)?a=l.countries:l&&"object"===typeof l&&(a=Object.entries(l).map(([n,e])=>({code:String(n).toUpperCase(),count:Number(e)||0}))),t.value=a.filter(n=>n&&n.code&&n.count>0).map(n=>({code:String(n.code).toUpperCase(),count:Number(n.count)||0})).sort((n,e)=>e.count-n.count).slice(0,n.topN)}catch{t.value=[]}}async function w(){if(n.collectApi)try{const e={href:location.href,referrer:document.referrer||"",tz:Intl.DateTimeFormat().resolvedOptions().timeZone||"",lang:navigator.language||"",ua:navigator.userAgent||""};await fetch(n.collectApi,{method:"POST",keepalive:!0,headers:{"Content-Type":"application/json"},body:JSON.stringify(e)})}catch{}}return(0,a.sV)(async()=>{m(),await(0,a.dY)(),S(),L(),k(),w()}),(0,a.xo)(()=>p()),{stars:e,updatedAt:l,countries:t,hidden:i,pos:o,onDragStart:d,onDragEnd:p,onClose:f,onReopen:h,formatNumber:v,regionName:b,flagEmoji:y,topN:n.topN}}};const me=(0,B.A)(he,[["render",ce],["__scopeId","data-v-02245dc6"]]);var ve=me,be={components:{ImpactMetrics:ve},name:"FloatingFeedback",props:{starsRepo:{type:String,default:"AI45Lab/Awesome-Trustworthy-Embodied-AI"},updatedRepo:{type:String,default:"AI45Lab/Safe-Trustworthy-EAI"},visitorsApi:{type:String,default:""},collectApi:{type:String,default:""},githubToken:{type:String,default:""},impactCtaText:{type:String,default:"If you have any comments on the advantages or disadvantages of this work, are willing to share your own research, or experience problems during use, please click the button on the right to contact us."},stackWidth:{type:Number,default:210},showImpact:{type:Boolean,default:!0},tooltip:{type:String,default:"Think your work fits this topic? Share it with us — we review regularly."},tooltipLocation:{type:String,default:"left"},buttonLabel:{type:String,default:"SHARE YOUR IDEA ✨"},helpLabel:{type:String,default:"MEET SOME TROUBLES"},variant:{type:String,default:"outlined"},color:{type:String,default:"primary"},rounded:{type:[Boolean,String,Number],default:"xl"},elevation:{type:[String,Number],default:0},size:{type:String,default:"default"},density:{type:String,default:"comfortable"},buttonHeight:{type:[String,Number],default:44},fixed:{type:Boolean,default:!0},align:{type:String,default:"right"},offsetX:{type:Number,default:24},offsetY:{type:Number,default:24},zIndex:{type:Number,default:2140},reserveBelow:{type:Number,default:160}},setup(n){const e="https://docs.google.com/forms/d/1LdZXCC7ufWNOgulFCSBNwlx0mXTF73tQ38Uyq-MxykQ",l="https://docs.google.com/forms/d/1ucW4QVJ77C7z9qEn34bsy_2YIYbF52z5l8Q-gsUKn6s",t=(0,a.EW)(()=>{const e={position:"fixed",zIndex:String(n.zIndex)},l=n.offsetY+n.reserveBelow;return"left"===n.align?e.left=n.offsetX+"px":e.right=n.offsetX+"px",e.bottom=l+"px",e.width=(n.stackWidth||320)+"px",e}),i=(0,un.KR)(!1),o=(0,un.KR)(!1),r=(0,un.KR)(""),s=(0,un.KR)(""),u="spc_verified_once",c=2592e6;function d(){try{const n=localStorage.getItem(u);if(!n)return!1;const e=JSON.parse(n).t||0;return Date.now()-e<c}catch{return!1}}function g(){try{localStorage.setItem(u,JSON.stringify({t:Date.now()}))}catch{}}const p=Math.floor(10+40*Math.random()),f=Math.floor(10+40*Math.random()),h=`What is ${p} + ${f}?`,m=(0,un.KR)("");function v(){d()?window.open(e,"_blank","noopener"):(s.value=e,i.value=!0)}function b(){d()?window.open(l,"_blank","noopener"):(s.value=l,i.value=!0)}function y(){i.value=!1}const A=(0,a.EW)(()=>String(Number(p+f))===String(m.value||"")),S=(0,a.EW)(()=>o.value?"Verifying...":"Verify & Continue");function L(){A.value&&(o.value=!0,setTimeout(()=>{g();const n=s.value||e;window.open(n,"_blank","noopener"),i.value=!1,o.value=!1},200))}return{helpUrl:l,fixedStyle:t,showVerify:i,verifying:o,errorMsg:r,basicQuestion:h,basicAnswer:m,openVerify:v,openVerifyTo:b,closeVerify:y,submitOrRefresh:L,canSubmit:A,actionLabel:S}}};const ye=(0,B.A)(be,[["render",Qn],["__scopeId","data-v-68ceeaf2"]]);var Ae=ye;function Se(){try{const n=Array.from(document.querySelectorAll(".spc-fixed"));if(n.length<2)return;const e=n=>{const e=n.style&&n.style.bottom?n.style.bottom:getComputedStyle(n).bottom,l=parseFloat((e||"0").replace("px",""))||0;return l};let l=null,t=null;if(n.forEach(n=>{const a=e(n);a>40?l=n:t=n}),!l||!t)return;const a=l.querySelector(".spc-btn"),i=t.querySelector(".spc-btn");if(!a||!i)return;const o=Math.round(a.getBoundingClientRect().width);o&&(i.style.width=o+"px")}catch(n){}}function Le(n){return Array.isArray(n)?n:Array.isArray(n?.value)?n.value:[]}(0,a.sV)(()=>{const n=()=>Se(),e=setInterval(n,150);setTimeout(()=>clearInterval(e),2e3),window.addEventListener("resize",n,{passive:!0})});(0,a.EW)(()=>{const n=Le("undefined"!==typeof filteredPapers?filteredPapers:[]),e=Le("undefined"!==typeof matrixFilteredRows?matrixFilteredRows:[]),l=Le("undefined"!==typeof papers?papers:[]);return n.length?n:e.length?e:l}),new Set(["a","an","the","and","or","for","of","on","in","to","with","by","at","from","about","as","is","are","be","was","were","this","that","these","those","we","you","they","he","she","it","into","over","under","between","against","without","within","across","per","via","using","use","than","then","there","here","our","your","their"]);var ke={components:{PaperCard:$,TagMatrix:Tn,PaperIntro:V,BibTeXSection:Xn,SubmitPaperClient:gn,FloatingFeedback:Ae},data(){return{matrixFilteredRows:null,tmxKey:0,papers:[],searchKeyword:"",selectedTags:[],filteredPapers:[],activeFilter:"",showPaperList:!0,showBackToTop:!1,intro:{title:"Towards Safe and Trustworthy Embodied AI: Foundations, Status, and Prospects",authors:[{name:"Xin Tan",homepage:"https://tanxincs.github.io/",symbol:"*"},{name:"Bangwei Liu",homepage:"#",symbol:"*"},{name:"Yicheng Bao",homepage:"#"},{name:"Qijian Tian",homepage:"https://fangzhou2000.github.io/"},{name:"Zhenkun Gao",homepage:"#"},{name:"Xiongbin Wu",homepage:"#"},{name:"Zhihao Luo",homepage:"#"},{name:"Sen Wang",homepage:"#"},{name:"Yuqi Zhang",homepage:"#"},{name:"Xuhong Wang",homepage:"https://wangxuhongcn.github.io",symbol:"†"},{name:"Chaochao Lu",homepage:"https://causallu.com/",symbol:"†"},{name:"Bowen Zhou",homepage:"https://scholar.google.com/citations?user=h3Nsz6YAAAAJ&hl=zh-CN&oi=ao",symbol:"†"}],affiliation:"Shanghai Artificial Intelligence Laboratory",links:{paper:"#"}}}},created(){this.processCSVData(Pn()),this.filteredPapers=this.papers},mounted(){window.addEventListener("scroll",this.handleScroll)},beforeDestroy(){window.removeEventListener("scroll",this.handleScroll)},computed:{__paper_rows(){const n=this.filteredPapers,e=this.matrixFilteredRows,l=this.papers;return Array.isArray(n)&&n.length?n:Array.isArray(e)&&e.length?e:Array.isArray(l)?l:[]}},methods:{_isStop(n){const e=(n||"").toString().toLowerCase();return e.length<=1||(this.__stopSet||(this.__stopSet=new Set(["a","an","the","of","for","and","to","in","on","with","via","by","at","from","into","over","under","using"])),this.__stopSet.has(e))},makeId(n){const e=n||{};return String(e.link||e["链接"]||e["cite{}"]||e.title||e["标题"]||"").toLowerCase().trim()},onMatrixFiltered(n){this.matrixFilteredRows=Array.isArray(n)?n.map(n=>n.raw||n):null,this.filterPapers()},_norm(n){return(n||"").toString().normalize("NFKD").replace(/[\u0300-\u036f]/g,"").toLowerCase().replace(/[_\-–—/\\\.,:;!?\(\)\[\]\{\}\"'`~@#$%^&*+=<>|]/g," ").replace(/\s+/g," ").trim()},_compact(n){return this._norm(n).replace(/\s+/g,"")},_lev(n,e){if(n=this._norm(n),e=this._norm(e),Math.abs(n.length-e.length)>1)return 99;if(n===e)return 0;let l=0,t=0,a=0;while(l<n.length&&t<e.length)if(n[l]!==e[t]){if(a++,a>1)return 99;n.length>e.length?l++:(n.length<e.length||l++,t++)}else l++,t++;return(l<n.length||t<e.length)&&a++,a},_tokenScore(n,e){if(!e||!n)return-1;const l=this._norm(n),t=this._compact(n),a=this._norm(e),i=this._compact(e),o=l.split(" ");return l.includes(a)?3:t.includes(i)?2.6:o.some(n=>n.startsWith(a))?2.2:a.length>=4&&o.some(n=>this._lev(n,a)<=1)?1.8:-1},_paperScore(n,e){const l=this._norm(e);let t=l.split(/\s+/).filter(Boolean),a=t.filter(n=>!this._isStop(n));a.length||(a=t);const i=n=>{const e=Array.isArray(n)?n:[n];let l=0;for(const t of e)for(const n of a){const e=this._tokenScore(t,n);e>0&&(l+=e)}return l};let o=0;const r=this._norm(n.title);return l&&(r===l?o+=2e3:r.startsWith(l)?o+=1200:(" "+r+" ").includes(" "+l+" ")?o+=900:r.includes(l)&&(o+=800)),o+=12*i(n.title),o+=6*(i(n.macros)+i(n.apps)+i(n.tasks)+i(n.method)),o+=4*i(n.keywords),o+=3*i(n.author),o+=1*i(n.contact),o},handleScroll(){this.showBackToTop=window.scrollY>300},scrollToTop(){window.scrollTo({top:0,behavior:"smooth"})},resetFilters(){this.searchKeyword="",this.selectedTags=[],this.activeFilter="",this.filteredPapers=this.papers,this.matrixFilteredRows=null,this.tmxKey+=1},clearSearch(){this.searchKeyword="",this.filterPapers()},clearTagFilter(){this.activeFilter="",this.filterPapers()},showAllPapers(){this.resetFilters()},processCSVData(n){this.papers=n.map(n=>{const e=n=>(n||"").toString().split(/[;、，,\s]+/).map(n=>n.trim()).filter(Boolean);return{title:n["标题"]||"",date:n["发表年月"]||"",author:n["一作"]||"",authorOrg:n["一作单位"]||"",principleTag:n["10大原则"]||"",stageTag:n["4个阶段"]||"",contact:n["通讯"]||"",contactOrg:n["通讯单位"]||"",macros:e(n["宏观维度"]),apps:e(n["应用维度"]),tasks:e(n["具身任务"]),method:e(n["方法论"]),link:n["链接"]||""}}).filter(n=>""!==n.title.trim())},filterPapers(){let n=Array.isArray(this.papers)?this.papers.slice():[];if(Array.isArray(this.matrixFilteredRows)&&this.matrixFilteredRows.length){const e=new Set(this.matrixFilteredRows.map(n=>this.makeId(n)));n=n.filter(n=>e.has(this.makeId(n)))}if(this.activeFilter&&this.activeFilter.includes("/")){const[e,l]=this.activeFilter.split("/");n=n.filter(n=>Array.isArray(n?.macros)&&n.macros.includes(e)&&Array.isArray(n?.apps)&&n.apps.includes(l))}const e=(this.searchKeyword||"").trim();if(!e)return void(this.filteredPapers=n);const l=Vn(n,e,60);console.info("[ProjectPage] search called:",e,"=>",l.length,"items"),this.filteredPapers=l},handleTagFilter(n,e){this.activeFilter=`${n}/${e}`,this.filterPapers()}},watch:{searchKeyword(){this.filterPapers()}}};const we=(0,B.A)(ke,[["render",h],["__scopeId","data-v-49f013bd"]]);var Me=we,Ee={name:"App",components:{ProjectPage:Me}};const xe=(0,B.A)(Ee,[["render",i]]);var Ce=xe,Re=(l(5524),l(5850)),Te=l(2802),Ie=l(1357);const Pe=(0,Re.$N)({components:Te,directives:Ie}),De=(0,t.Ef)(Ce);De.use(Pe),De.mount("#app")}},e={};function l(t){var a=e[t];if(void 0!==a)return a.exports;var i=e[t]={exports:{}};return n[t].call(i.exports,i,i.exports,l),i.exports}l.m=n,function(){var n=[];l.O=function(e,t,a,i){if(!t){var o=1/0;for(c=0;c<n.length;c++){t=n[c][0],a=n[c][1],i=n[c][2];for(var r=!0,s=0;s<t.length;s++)(!1&i||o>=i)&&Object.keys(l.O).every(function(n){return l.O[n](t[s])})?t.splice(s--,1):(r=!1,i<o&&(o=i));if(r){n.splice(c--,1);var u=a();void 0!==u&&(e=u)}}return e}i=i||0;for(var c=n.length;c>0&&n[c-1][2]>i;c--)n[c]=n[c-1];n[c]=[t,a,i]}}(),function(){l.n=function(n){var e=n&&n.__esModule?function(){return n["default"]}:function(){return n};return l.d(e,{a:e}),e}}(),function(){l.d=function(n,e){for(var t in e)l.o(e,t)&&!l.o(n,t)&&Object.defineProperty(n,t,{enumerable:!0,get:e[t]})}}(),function(){l.g=function(){if("object"===typeof globalThis)return globalThis;try{return this||new Function("return this")()}catch(n){if("object"===typeof window)return window}}()}(),function(){l.o=function(n,e){return Object.prototype.hasOwnProperty.call(n,e)}}(),function(){l.r=function(n){"undefined"!==typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(n,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(n,"__esModule",{value:!0})}}(),function(){l.p="/Awesome-Trustworthy-Embodied-AI/"}(),function(){var n={524:0};l.O.j=function(e){return 0===n[e]};var e=function(e,t){var a,i,o=t[0],r=t[1],s=t[2],u=0;if(o.some(function(e){return 0!==n[e]})){for(a in r)l.o(r,a)&&(l.m[a]=r[a]);if(s)var c=s(l)}for(e&&e(t);u<o.length;u++)i=o[u],l.o(n,i)&&n[i]&&n[i][0](),n[i]=0;return l.O(c)},t=self["webpackChunkproject_page"]=self["webpackChunkproject_page"]||[];t.forEach(e.bind(null,0)),t.push=e.bind(null,t.push.bind(t))}();var t=l.O(void 0,[504],function(){return l(3632)});t=l.O(t)})();
//# sourceMappingURL=app.eadccb53.js.map